{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"bf-segment-attack-ncvoter-real.ipynb","provenance":[],"collapsed_sections":["97WhQAHmkXtO"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Diclaimer\r\n","\r\n","An atom BF based attack on BF segments\r\n","\r\n","Original author: Peter Christen\r\n","\r\n","Contact: peter.christen@anu.edu.au\r\n","\r\n","School of Computing, The Australian National University, Canberra, ACT, 2600\r\n","\r\n","Python 3 migration: Thiago Nóbrega <<thiagonobrega@gmail.com>>\r\n","\r\n","---\r\n","Copyright 2021 Australian National University and others.\r\n","All Rights reserved.\r\n","\r\n","---\r\n","\r\n","This program is free software: you can redistribute it and/or modify it under\r\n","the terms of the GNU General Public License as published by the Free Software\r\n","Foundation, either version 3 of the License, or (at your option) any later\r\n","version.\r\n","\r\n","This program is distributed in the hope that it will be useful, but WITHOUT\r\n","ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\r\n","FOR A PARTICULAR PURPOSE. See the GNU General Public License for more\r\n","details.\r\n","\r\n","You should have received a copy of the GNU General Public License along with\r\n","this program.  If not, see <http://www.gnu.org/licenses/>.\r\n","\r\n","---"],"metadata":{"id":"gqZisHQOP612"}},{"cell_type":"markdown","source":["**Enviroment setup**\r\n","---\r\n","\r\n","-   Install Libs\r\n","-   Load Dataset\r\n","-   Load libs"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["!pip install numpy &> /dev/null\r\n","!pip install bitarray &> /dev/null\r\n","!pip install hashlib &> /dev/null\r\n","!pip install xxhash &> /dev/null\r\n","\r\n","!rm /tmp/ncvoter* &> /dev/null\r\n","!rm /tmp/brpoli* &> /dev/null\r\n","\r\n","!wget -P /tmp/ 'https://github.com/thiagonobrega/bcpprl-simplified/raw/master/dataset/ncvoter-20140619-temporal-balanced-ratio-1to1-a.csv.gz' &> /dev/null\r\n","!wget -P /tmp/ 'https://github.com/thiagonobrega/bcpprl-simplified/raw/master/dataset/ncvoter-20140619-temporal-balanced-ratio-1to1-b.csv.gz' &> /dev/null\r\n","!wget -P /tmp/ 'https://github.com/thiagonobrega/bcpprl-simplified/raw/master/dataset/sample-ncvoter-20140619-temporal-balanced-ratio-1to1-b.csv.gz' &> /dev/null\r\n","!wget -P /tmp/ 'https://github.com/thiagonobrega/bcpprl-simplified/raw/master/dataset/ncvr-splits/sample-ncvoter-20140619-temporal-balanced-ratio-1to1-b-bk_NM-split2.csv.gz' &> /dev/null\r\n","!wget -P /tmp/ 'https://github.com/thiagonobrega/bcpprl-simplified/raw/master/dataset/ncvr-splits/sample-ncvoter-20140619-temporal-balanced-ratio-1to1-b-bk_NM-split3.csv.gz' &> /dev/null\r\n","!wget -P /tmp/ 'https://github.com/thiagonobrega/bcpprl-simplified/raw/master/dataset/ncvr-splits/sample-ncvoter-20140619-temporal-balanced-ratio-1to1-b-bk_NM-split4.csv.gz' &> /dev/null\r\n","!wget -P /tmp/ 'https://github.com/thiagonobrega/bcpprl-simplified/raw/master/dataset/ncvr-splits/sample-ncvoter-20140619-temporal-balanced-ratio-1to1-b-bk_NM-split5.csv.gz' &> /dev/null\r\n","!wget -P /tmp/ 'https://github.com/thiagonobrega/bcpprl-simplified/raw/master/dataset/ncvr-splits/sample-ncvoter-20140619-temporal-balanced-ratio-1to1-b-bk_NM-split6.csv.gz' &> /dev/null\r\n","\r\n","!wget -P /tmp/ 'https://github.com/thiagonobrega/bcpprl-simplified/raw/master/dataset/brpoliticians-federal-2014-all-1to1-a.csv.gz' &> /dev/null\r\n","!wget -P /tmp/ 'https://github.com/thiagonobrega/bcpprl-simplified/raw/master/dataset/brpoliticians-federal-2018-all-1to1-b.csv.gz' &> /dev/null\r\n","!wget -P /tmp/ 'https://github.com/thiagonobrega/bcpprl-simplified/raw/master/dataset/brpoliticians-federal-2018-5p-i2.csv.gz' &> /dev/null\r\n","!wget -P /tmp/ 'https://github.com/thiagonobrega/bcpprl-simplified/raw/master/dataset/brpoliticians-federal-2018-5p-i3.csv.gz' &> /dev/null\r\n","!wget -P /tmp/ 'https://github.com/thiagonobrega/bcpprl-simplified/raw/master/dataset/brpoliticians-federal-2018-5p-i4.csv.gz' &> /dev/null\r\n","!wget -P /tmp/ 'https://github.com/thiagonobrega/bcpprl-simplified/raw/master/dataset/brpoliticians-federal-2018-5p-i5.csv.gz' &> /dev/null\r\n","!wget -P /tmp/ 'https://github.com/thiagonobrega/bcpprl-simplified/raw/master/dataset/brpoliticians-federal-2018-5p-i6.csv.gz' &> /dev/null\r\n","\r\n","\r\n","import csv\r\n","import gzip\r\n","import hashlib\r\n","import math\r\n","import random\r\n","import os\r\n","import sys\r\n","import time\r\n","\r\n","import bitarray\r\n","import numpy\r\n","import xxhash\r\n","import pandas as pd"],"outputs":[],"metadata":{"id":"ibwXOn_-hOlP","executionInfo":{"status":"ok","timestamp":1630716157612,"user_tz":180,"elapsed":23380,"user":{"displayName":"Thiago Nóbrega","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_-BHXOY4rTl8cwozc96ukJW5fc7kC04rST8UwDgY=s64","userId":"01534443958959954150"}}}},{"cell_type":"markdown","source":["Configure google drive to save the output"],"metadata":{"id":"NQ1V4eaaQY0Y"}},{"cell_type":"code","execution_count":null,"source":["from google.colab import drive\r\n","drive.mount(\"/gdrive/\")\r\n","RESULTS_PATH='/gdrive/My Drive/Colab Notebooks/bc-pprlSegmentAttack/results/'"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WMDdCL5gyE2j","executionInfo":{"status":"ok","timestamp":1630716269825,"user_tz":180,"elapsed":19963,"user":{"displayName":"Thiago Nóbrega","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_-BHXOY4rTl8cwozc96ukJW5fc7kC04rST8UwDgY=s64","userId":"01534443958959954150"}},"outputId":"68798d6c-dc1a-467f-87a7-dfd8b9252c67"}},{"cell_type":"markdown","source":["## Metodos"],"metadata":{"id":"97WhQAHmkXtO"}},{"cell_type":"code","execution_count":null,"source":["def load_data_set_extract_q_grams(file_name, rec_id_col, use_attr_list,\r\n","                                  col_sep_char, header_line, q):\r\n","  \"\"\"Load the given file, extract selected attributes, and convert each unique\r\n","     attribute value into a q-gram set.\r\n","\r\n","     Return two dictionaries, one with attribute values as keys and their\r\n","     q-gram set as values. The second dictionary has q-grams as keys and for\r\n","     each a set of all the plain-text values that contain this q-gram.\r\n","\r\n","     The function also returns the average number of q-grams per attribute\r\n","     value, and the set of all unique q-grams that occurred in any values.\r\n","     Also returns a string with the names of the encoded attributes.\r\n","  \"\"\"\r\n","\r\n","  start_time = time.time()\r\n","\r\n","  \r\n","  if (file_name.endswith('gz')):\r\n","    #modified\r\n","    f = gzip.open(DATASET_PATH+file_name,mode='rt',errors=\"ignore\")\r\n","  else:\r\n","    f = open(DATASET_PATH+file_name)\r\n","\r\n","  csv_reader = csv.reader(f, delimiter=col_sep_char)\r\n","\r\n","  print('Load data set from file:', file_name)\r\n","  print('  Attribute separator: %c' % (col_sep_char))\r\n","  if (header_line == True):\r\n","    # modificado\r\n","    # header_list = csv_reader.next()\r\n","    header_list = next(csv_reader)\r\n","    print('  Header line:', header_list)\r\n","\r\n","  use_attr_name_list = []\r\n","\r\n","  if (header_line == False):\r\n","    print('  Record identifier attribute number:', rec_id_col)\r\n","  else:\r\n","    print('  Record identifier attribute:', header_list[rec_id_col])\r\n","    print('    Attributes to use:')\r\n","    for attr_num in use_attr_list:\r\n","      use_attr_name = header_list[attr_num]\r\n","      print(use_attr_name)\r\n","      use_attr_name_list.append(use_attr_name)\r\n","    print('')\r\n","  print('')\r\n","  print('  Extract q-grams with q=%d' % (q))\r\n","  print('')\r\n","\r\n","  encoded_attr_name_str = ';'.join(use_attr_name_list)\r\n","\r\n","  qm1 = q-1  # Shorthand\r\n","\r\n","  attr_val_q_gram_dict = {}\r\n","  q_gram_attr_val_dict = {}\r\n","  all_q_gram_set =       set()\r\n","\r\n","  num_q_gram_per_attr_val_list = []  # To calculate statistics\r\n","\r\n","  rec_num = 0\r\n","\r\n","  for rec_list in csv_reader:\r\n","    rec_num += 1\r\n","\r\n","    if (rec_num % 100000 == 0):\r\n","      time_used = time.time() - start_time\r\n","      print('  Processed %d records in %.3f sec (%.3f msec average)' % \\\r\n","            (rec_num, time_used, 1000.0*time_used/rec_num))\r\n","\r\n","    attr_val_list = []  # The attribute values to be converted into q-grams\r\n","\r\n","    for attr_num in use_attr_list:\r\n","      attr_val_list.append(rec_list[attr_num].strip().lower())\r\n","\r\n","    # Concatenate with whitespaces and keep as one string per record\r\n","    #\r\n","    attr_val = ' '.join(attr_val_list).strip()\r\n","\r\n","    # Encode each attribute value once\r\n","    #\r\n","    if (attr_val not in attr_val_q_gram_dict):\r\n","\r\n","      attr_q_gram_set = set([attr_val[i:i+q] for i in range(len(attr_val)-qm1)])\r\n","\r\n","      if (len(attr_q_gram_set) > 0):\r\n","\r\n","        # Keep the q-gram set for this attribute value\r\n","        #\r\n","        attr_val_q_gram_dict[attr_val] = attr_q_gram_set\r\n","        num_q_gram_per_attr_val_list.append(len(attr_q_gram_set))\r\n","\r\n","        # Keep the attribute value for each of its q-grams\r\n","        #\r\n","        for q_gram in attr_q_gram_set:\r\n","          q_gram_attr_val_set = q_gram_attr_val_dict.get(q_gram, set())\r\n","          q_gram_attr_val_set.add(attr_val)\r\n","          q_gram_attr_val_dict[q_gram] = q_gram_attr_val_set\r\n","\r\n","          all_q_gram_set.add(q_gram)  # And keep all unique q-grams\r\n","\r\n","  time_used = time.time() - start_time\r\n","  print('  Processed %d records in %.2f sec (%.2f msec average)' % \\\r\n","        (rec_num, time_used, 1000.0*time_used/rec_num))\r\n","\r\n","  num_attr_val_q_gram_list = []\r\n","  \r\n","  #modified to python3\r\n","  for attr_val_set in q_gram_attr_val_dict.items():\r\n","  # for attr_val_set in q_gram_attr_val_dict.itervalues():\r\n","    num_attr_val_q_gram_list.append(len(attr_val_set))\r\n","\r\n","  # print\r\n","  print('  Found %d unique attribute values' % (len(attr_val_q_gram_dict)))\r\n","  print('')\r\n","  print('  Number of attribute values per q-gram:')\r\n","  print('    Minimum: %d' % (min(num_attr_val_q_gram_list)));\r\n","  print('    Average: %.1f' % (numpy.mean(num_attr_val_q_gram_list)))\r\n","  print('    Median:  %d' % (numpy.median(num_attr_val_q_gram_list)))\r\n","  print('    Maximum: %d' % (max(num_attr_val_q_gram_list)))\r\n","  \r\n","  avr_num_q_gram = numpy.mean(num_q_gram_per_attr_val_list)\r\n","\r\n","  print('  Average number of q-grams per attribte value: %.2f' % \\\r\n","        (avr_num_q_gram))\r\n","  print('    Minimum and maximum number of q-grams: %d / %d' % \\\r\n","        (min(num_q_gram_per_attr_val_list), max(num_q_gram_per_attr_val_list)))\r\n","  print('  Total number of unique q-grams in all values: %d' % \\\r\n","        (len(all_q_gram_set)))\r\n","  print\r\n","\r\n","  return attr_val_q_gram_dict, q_gram_attr_val_dict, avr_num_q_gram, \\\r\n","         all_q_gram_set, encoded_attr_name_str\r\n","\r\n","def gen_bloom_filter_dict(attr_val_q_gram_dict, hash_type, bf_len,\r\n","                          num_hash_funct):\r\n","  \"\"\"Encode the q-gram sets in the given dictionary into Bloom filters of the\r\n","     given length using the given number of hash functions.\r\n","\r\n","     Return a dictionary with keys being q-gram sets (as tuples) and values\r\n","     being Bloom filters (bit arrays), a dictionary of bit positions (keys)\r\n","     and which q-grams were hashed to then (values), and a dictionary of atom\r\n","     BFs which encode a single q-gram (one such BF for each unique q-gram).\r\n","  \"\"\"\r\n","\r\n","  start_time = time.time()\r\n","\r\n","  print('Generate Bloom filter bit-patterns for %d q-gram sets' % \\\r\n","        (len(attr_val_q_gram_dict)))\r\n","  print('  Bloom filter length:          ', bf_len)\r\n","  print('  Number of hash functions used:', num_hash_funct)\r\n","  print('  Hashing type used:            ', \\\r\n","        {'dh':'Double hashing', 'rh':'Random hashing', \r\n","         'xrh':'xHash Random hashing', 'xbf': 'xHash Xor Folding'}[hash_type])\r\n","  print('')\r\n","\r\n","  bf_dict = {}  # One BF per q-gram set\r\n","\r\n","  bit_pos_q_gram_dict = {}  # Bit pos. are keys, q-grams hashed to them values\r\n","\r\n","  atom_bf_dict = {}  # Keys are q-grams, values their atom BF\r\n","  \r\n","  num_q_gram_set = 0  # Count how many q-gram sets processed\r\n","\r\n","  bf_len_m1 = bf_len-1\r\n","\r\n","  for (attr_val, q_gram_set) in attr_val_q_gram_dict.items():\r\n","    num_q_gram_set += 1\r\n","\r\n","    if (num_q_gram_set % 10000 == 0):\r\n","      time_used = time.time() - start_time\r\n","      print('  Generated %d Bloom filters in %d sec (%.3f msec average)' % \\\r\n","            (num_q_gram_set, time_used, 1000.0*time_used/num_q_gram_set))\r\n","\r\n","    rec_bf = bitarray.bitarray(bf_len)\r\n","    rec_bf.setall(0)\r\n","\r\n","    for q_gram in q_gram_set:  # Hash all q-grams into bits in the BF\r\n","\r\n","      if (q_gram not in atom_bf_dict):  # We need to generate the atom BF\r\n","        atom_bf = bitarray.bitarray(bf_len)\r\n","        atom_bf.setall(0)\r\n","\r\n","      \r\n","      ###\r\n","      ### Parte importante para o algoritmo utiliza apenas duas funções como \r\n","      ### como hash, ou seja apenas duas funções...\r\n","      ###\r\n","      if (hash_type == 'dh'):  # Double hashing\r\n","        hex_str1 = BF_HASH_FUNCT1(q_gram).hexdigest()\r\n","        int1 =     int(hex_str1, 16)\r\n","        hex_str2 = BF_HASH_FUNCT2(q_gram).hexdigest()\r\n","        int2 =     int(hex_str2, 16)\r\n","\r\n","        for i in range(1,num_hash_funct+1):  # Ensure i is not 0\r\n","          pos = int((int1 + i*int2) % bf_len)\r\n","          rec_bf[pos] = 1\r\n","\r\n","          if (q_gram not in atom_bf_dict):\r\n","            atom_bf[pos] = 1\r\n","\r\n","          bit_pos_q_gram_set = bit_pos_q_gram_dict.get(pos, set())\r\n","          bit_pos_q_gram_set.add(q_gram)\r\n","          bit_pos_q_gram_dict[pos] = bit_pos_q_gram_set\r\n","\r\n","      ###\r\n","      ### o utilizado\r\n","      ###\r\n","      elif (hash_type == 'rh'):  # Random hashing\r\n","        random_seed = random.seed(q_gram)\r\n","\r\n","        for i in range(num_hash_funct):\r\n","          pos = random.randint(0, bf_len_m1)\r\n","          rec_bf[pos] = 1\r\n","\r\n","          if (q_gram not in atom_bf_dict):\r\n","            atom_bf[pos] = 1\r\n","\r\n","          bit_pos_q_gram_set = bit_pos_q_gram_dict.get(pos, set())\r\n","          bit_pos_q_gram_set.add(q_gram)\r\n","          bit_pos_q_gram_dict[pos] = bit_pos_q_gram_set\r\n","\r\n","      ###\r\n","      ### Funcao hash que utilizei\r\n","      ###\r\n","      elif (hash_type == 'xrh'):  # Xhash Random hashing\r\n","        for seed in range(num_hash_funct):\r\n","          pos=xxhash.xxh64(q_gram, seed=seed).intdigest() % bf_len\r\n","          rec_bf[pos] = 1\r\n","        \r\n","          if (q_gram not in atom_bf_dict):\r\n","            atom_bf[pos] = 1\r\n","          \r\n","          bit_pos_q_gram_set = bit_pos_q_gram_dict.get(pos, set())\r\n","          bit_pos_q_gram_set.add(q_gram)\r\n","          bit_pos_q_gram_dict[pos] = bit_pos_q_gram_set\r\n","\r\n","      ###\r\n","      ### XOR FOlding\r\n","\r\n","      elif (hash_type == 'xbf'):  # Xhash Random hashing\r\n","        for seed in range(num_hash_funct):\r\n","          \r\n","          fold_location = int(round( bf_len / 2 )) # xor pos\r\n","          pos=xxhash.xxh64(q_gram, seed=seed).intdigest() % fold_location\r\n","          rec_bf[pos] = 1\r\n","        \r\n","          if (q_gram not in atom_bf_dict):\r\n","            atom_bf[pos] = 1\r\n","          \r\n","          bit_pos_q_gram_set = bit_pos_q_gram_dict.get(pos, set())\r\n","          bit_pos_q_gram_set.add(q_gram)\r\n","          bit_pos_q_gram_dict[pos] = bit_pos_q_gram_set\r\n","\r\n","      else:  # Should not happend\r\n","        raise Exception(hash_type)\r\n","\r\n","      if (q_gram not in atom_bf_dict):\r\n","        atom_bf_dict[q_gram] = atom_bf\r\n","\r\n","    if (hash_type == 'xbf'):\r\n","      fold_location = int(round( bf_len / 2 )) # xor pos\r\n","      bf_dict[attr_val] = rec_bf[0:fold_location]\r\n","    else:\r\n","      bf_dict[attr_val] = rec_bf\r\n","\r\n","\r\n","  num_q_gram_at_pos_list = []  # To calculate statistics\r\n","\r\n","  for pos in sorted(bit_pos_q_gram_dict.keys()):\r\n","    num_q_gram_at_pos_list.append(len(bit_pos_q_gram_dict[pos]))\r\n","\r\n","  print('  Number of q-grams assigned to bit positions:')\r\n","  print('    Minimum: %d' % (min(num_q_gram_at_pos_list)))\r\n","  print('    Average: %.1f' % (numpy.mean(num_q_gram_at_pos_list)))\r\n","  print('    Median:  %d' % (numpy.median(num_q_gram_at_pos_list)))\r\n","  print('    Maximum: %d' % (max(num_q_gram_at_pos_list)))\r\n","  print('  Generated atom BFs for %d q-grams' % (len(atom_bf_dict)))\r\n","  print('')\r\n","\r\n","  assert len(bf_dict) == len(attr_val_q_gram_dict)\r\n","\r\n","  return bf_dict, bit_pos_q_gram_dict, atom_bf_dict\r\n","\r\n","######\r\n","#####\r\n","####\r\n","### SBF adptation\r\n","##\r\n","#\r\n","\r\n","def get_bf_segments(bf_dict, bf_seg_len):\r\n","  \"\"\"From the given Bloom filter dictionary, extract for each BF the segment\r\n","     of the required length.\r\n","\r\n","     Return a new dictionary with the same keys and values being the BF\r\n","     segments.\r\n","  \"\"\"\r\n","\r\n","  bf_seg_dict = {}\r\n","\r\n","  for (key_val, bf) in bf_dict.items():\r\n","    bf_seg_dict[key_val] = bf[:bf_seg_len]\r\n","    assert len(bf_seg_dict[key_val]) == bf_seg_len\r\n","\r\n","  print('Extracted BF segments of length %d bits from %d BFs' % \\\r\n","        (bf_seg_len, len(bf_seg_dict)))\r\n","  print\r\n","\r\n","  return bf_seg_dict\r\n","\r\n","def bf_segment_get_num_q_gram(bit_pos_q_gram_dict, bf_seg_len, num_hash_funct):\r\n","  \"\"\"Get the number of unique q-grams that have been hashed into at least one\r\n","     bit position in the given BF segment. For each of these q-grams count in\r\n","     how many bit positions the q-gram is encoded.\r\n","\r\n","     Returns the number of q-grams hashed into bits in the segment, the\r\n","     minimum, average, median, and maximum number of times a q-gram is hashed\r\n","     into a bit in the segment.\r\n","  \"\"\"\r\n","\r\n","  print('Get the number of unique q-grams that have been hashed into at ' + \\\r\n","        'least one bit position in the given BF segment.')\r\n","  print('  Length of BF segment:', bf_seg_len)\r\n","\r\n","\r\n","  seg_q_gram_count_dict = {}  # Number of positions each q-gram is encoded into\r\n","  all_q_gram_count_dict = {}  # Should be number of hash functions for this\r\n","\r\n","  for (pos, pos_q_gram_set) in bit_pos_q_gram_dict.items():\r\n","    for q_gram in pos_q_gram_set:\r\n","      all_q_gram_count_dict[q_gram] = all_q_gram_count_dict.get(q_gram,0) + 1\r\n","      if (pos < bf_seg_len):\r\n","        seg_q_gram_count_dict[q_gram] = seg_q_gram_count_dict.get(q_gram,0) + 1\r\n","\r\n","  seg_q_gram_count_list = list(seg_q_gram_count_dict.values())\r\n","  all_q_gram_count_list = list(all_q_gram_count_dict.values())\r\n","\r\n","  # Check for all that no count is larger than number of hash functions used\r\n","  #\r\n","  assert max(all_q_gram_count_list) <= num_hash_funct\r\n","\r\n","  print('  There are %d unique q-grams encoded into bit positions in this ' \\\r\n","        % (len(seg_q_gram_count_dict)) + 'segment, from a total of %d ' % \\\r\n","          (len(all_q_gram_count_dict))+'q-grams encoded')\r\n","  print('    Number of positions each q-gram is hashed into in segment and ' + \\\r\n","        'full BF:')\r\n","  print('      (note for the full BF this is likely less than the number of' + \\\r\n","        ' hash functions (%d) due to collisions)' % (num_hash_funct))\r\n","  print('    Minimum: %d / %d' % (min(seg_q_gram_count_list),\r\n","                                  min(all_q_gram_count_list)))\r\n","  print('    Average: %.1f / %.1f' % (numpy.mean(seg_q_gram_count_list),\r\n","                                      numpy.mean(all_q_gram_count_list)))\r\n","  print('    Median:  %d / %d' % (numpy.median(seg_q_gram_count_list),\r\n","                                  numpy.median(all_q_gram_count_list)))\r\n","  print('    Maximum: %d / %d' % (max(seg_q_gram_count_list),\r\n","                                  max(all_q_gram_count_list)))\r\n","  print('')\r\n","\r\n","  return len(seg_q_gram_count_dict), min(seg_q_gram_count_list), \\\r\n","         numpy.mean(seg_q_gram_count_list), \\\r\n","         numpy.median(seg_q_gram_count_list), max(seg_q_gram_count_list)\r\n","\r\n","def bf_segment_atom_attack(ot_bf_seg_dict, my_atom_bf_seg_dict,\r\n","                           my_bit_pos_q_gram_dict, q_gram_attr_val_dict,\r\n","                           num_bf_to_attack):\r\n","  \"\"\"Use atom BFs (for individual q-grams) to identify which q-grams can be\r\n","     encoded in a BF segment of the other DO. Then further filter possible\r\n","     q-grams by removing those known to be hashed to a bit position where a\r\n","     BF segment has a 0-bit (these could be false positive atoms due to\r\n","     collisions). Finally for each BF segment find the attribute value(s)\r\n","     that contain(s) the' largest number of the identified q-grams.\r\n","\r\n","     Returns the number of correct 1-1, correct 1-many, wrong, and no matches\r\n","     where the sum of these numbers will be equal to 'num_bf_to_attack' (or\r\n","     equal to all BFs in the given BF dictionary if 'num_bf_to_attack' is\r\n","     given as 'all')\r\n","  \"\"\"\r\n","\r\n","  start_time = time.time()\r\n","\r\n","  ot_num_bf_seg = len(ot_bf_seg_dict)\r\n","\r\n","  if (num_bf_to_attack == 'all'):\r\n","    ot_num_bf_to_attack = ot_num_bf_seg\r\n","  else:\r\n","    assert num_bf_to_attack <= ot_num_bf_seg\r\n","    ot_num_bf_to_attack = num_bf_to_attack\r\n","\r\n","  ot_attr_val_to_attack_list = random.sample(ot_bf_seg_dict.keys(),\r\n","                                             ot_num_bf_to_attack)\r\n","\r\n","  print('Attack BF segments by atom BF matching BF segment bit patterns')\r\n","  print('  Number of other DO BF segments:   ', ot_num_bf_seg)\r\n","  print('    Number of BF segments to attack:', ot_num_bf_to_attack)\r\n","  print('  Number of atom BFs:               ', len(my_atom_bf_seg_dict))\r\n","  print('')\r\n","\r\n","  num_bf = 0  # Count how many BFs processed\r\n","\r\n","  num_corr_1_1_attr_matches = 0  # Counts of how good the attack is\r\n","  num_corr_1_m_attr_matches = 0\r\n","  num_wrong_1_matches =       0\r\n","  num_wrong_m_matches =       0\r\n","  num_no_matches =            0\r\n","  records_matched =           [] # the reidentified entities\r\n","\r\n","  # How many more q-grams identified versus how many are encoded\r\n","  #\r\n","  diff_true_poss_size = []\r\n","\r\n","  # For the second (filtering) step, count total reduction in possible q-grams\r\n","  #\r\n","  zero_bit_num_q_gram_removed = 0\r\n","\r\n","  # Loop over all other BF segments and find the possible q-grams that could\r\n","  # have been encoded into a segment\r\n","  #\r\n","  for ot_attr_val in ot_attr_val_to_attack_list:\r\n","    ot_bf_seg = ot_bf_seg_dict[ot_attr_val]\r\n","    ot_true_q_gram_set = ot_attr_val_q_gram_dict[ot_attr_val] # True q-gram set\r\n","\r\n","    num_bf += 1\r\n","\r\n","    if (num_bf % 1000 == 0):\r\n","      time_used = time.time() - start_time\r\n","      print('  Analysed %d Bloom filters in %d sec (%.3f msec average)' % \\\r\n","            (num_bf, time_used, 1000.0*time_used/num_bf))\r\n","\r\n","    poss_q_gram_set = set()  # The q-grams possibly encoded in the BF segment\r\n","\r\n","    # Step 1: The set of possibly encoded q-grams based on atoms that have all\r\n","    # their 1-bits set in the BF segment\r\n","    #\r\n","    for (atom_q_gram, atom_bf_seg) in my_atom_bf_seg_dict.items():\r\n","      if (atom_bf_seg & ot_bf_seg == atom_bf_seg):\r\n","        poss_q_gram_set.add(atom_q_gram)\r\n","\r\n","    # Step 2: For 0-bits in the BF segment, remove from the set of possible\r\n","    # q-grams those that we know have been hashed to that position (these\r\n","    # possible q-grams could have been identified as atoms due to collisions)\r\n","    #\r\n","    for (pos, bit) in enumerate(ot_bf_seg):\r\n","      if (bit == False):  # A 0-bit\r\n","\r\n","        # Get the q-grams we know have been hashed to this position\r\n","        #\r\n","        my_pos_q_gram_set = my_bit_pos_q_gram_dict[pos]\r\n","\r\n","        # Remove them from the set of possible q-grams\r\n","        #\r\n","        for q_gram in my_pos_q_gram_set:\r\n","          if (q_gram in poss_q_gram_set):\r\n","            poss_q_gram_set.remove(q_gram)\r\n","            zero_bit_num_q_gram_removed += 1\r\n","            print('removed', q_gram)\r\n","\r\n","    # Step 3: Get possible attribute values that contain one or more of the\r\n","    # possible q-grams, and select the attribute value with the largest number\r\n","    # of such q-grams to be the most likely value encoded in this BF\r\n","    #\r\n","    attr_val_accu = {}  # Keys will be attribute values, values counts of how\r\n","                        # many q-grams they have in the set of possible q-grams\r\n","\r\n","    # TODO: this below process could be rather slow... need to improve *******\r\n","\r\n","    max_count = -1\r\n","\r\n","    for q_gram in poss_q_gram_set:\r\n","      q_gram_attr_val_set = q_gram_attr_val_dict[q_gram]\r\n","      for attr_val in q_gram_attr_val_set:\r\n","        count = attr_val_accu.get(attr_val, 0) + 1\r\n","        attr_val_accu[attr_val] = count\r\n","        max_count = max(count, max_count)\r\n","\r\n","    found_attr_val_list = []\r\n","\r\n","    # Find those attribute value(s) that have the maximum count (as how many\r\n","    # q-grams they have in the set of possible q-grams)\r\n","    #\r\n","    for (attr_val, count) in attr_val_accu.items():\r\n","      if (count == max_count):\r\n","        found_attr_val_list.append(attr_val)\r\n","\r\n","#    print '  Most likely encoded attribute values(s):', found_attr_val_list\r\n","#    print '    True encoded attribute values:', ot_attr_val\r\n","\r\n","    # Check if the found value(s) are correct:\r\n","    #\r\n","    if (len(found_attr_val_list) == 0):\r\n","      num_no_matches += 1\r\n","    elif (ot_attr_val in found_attr_val_list):\r\n","      if (len(found_attr_val_list) == 1):\r\n","        num_corr_1_1_attr_matches += 1\r\n","        records_matched.append(ot_attr_val) # reidentified entities list\r\n","      else:\r\n","        num_corr_1_m_attr_matches += 1\r\n","    else:\r\n","      if (len(found_attr_val_list) == 1):\r\n","        num_wrong_1_matches += 1\r\n","      else:\r\n","        num_wrong_m_matches += 1\r\n","  print('')\r\n","\r\n","  print('  Filtering step using 0-bits removed a total %d q-grams' % \\\r\n","        (zero_bit_num_q_gram_removed) + ' from sets of possible q-grams')\r\n","  print('')\r\n","\r\n","  print('  Number of correct attribute values identified 1-to-1:    ' + \\\r\n","        '%d (%.2f%%)' % (num_corr_1_1_attr_matches, 100.0 * \\\r\n","                         num_corr_1_1_attr_matches/ot_num_bf_to_attack))\r\n","  print('  Number of correct attribute values identified 1-to-many: ' + \\\r\n","        '%d (%.2f%%)' % (num_corr_1_m_attr_matches, 100.0 * \\\r\n","                         num_corr_1_m_attr_matches/ot_num_bf_to_attack))\r\n","  print('  Number of wrong 1-to-1 attribute values identified:      ' + \\\r\n","        '%d (%.2f%%)' % (num_wrong_1_matches, 100.0 * \\\r\n","                         num_wrong_1_matches/ot_num_bf_to_attack))\r\n","  print('  Number of wrong 1-to-many attribute values identified:   ' + \\\r\n","        '%d (%.2f%%)' % (num_wrong_m_matches, 100.0 * \\\r\n","                         num_wrong_m_matches/ot_num_bf_to_attack))\r\n","  print('  Number of no attribute values identified:                ' + \\\r\n","        '%d (%.2f%%)' % (num_no_matches, 100.0 * \\\r\n","                         num_no_matches/ot_num_bf_to_attack))\r\n","  print('')\r\n","\r\n","  assert (num_corr_1_1_attr_matches + num_corr_1_m_attr_matches + \\\r\n","         num_wrong_1_matches + num_wrong_m_matches + num_no_matches) == \\\r\n","         ot_num_bf_to_attack\r\n","\r\n","  return (num_corr_1_1_attr_matches, num_corr_1_m_attr_matches, \\\r\n","          num_wrong_1_matches, num_wrong_m_matches, num_no_matches , \\\r\n","          records_matched)\r\n","\r\n","def profile_reidentied_records(DATASET_PATH,my_data_set_name,ot_data_set_name,\r\n","                               col_sep_char,rec_id_col,attr_list):\r\n","  \"\"\"This method uses the list of identified values to profile records\r\n","      in the original datasets.\r\n","\r\n","     Returns the number of reidentified records (1:1), how many of these \r\n","     are matches and nonmateches.\r\n","  \"\"\"\r\n","\r\n","  otds = pd.read_csv(DATASET_PATH+ot_data_set_name,sep=col_sep_char,\r\n","                     encoding = 'unicode_escape', engine ='python',\r\n","                     quoting=3)\r\n","  myds = pd.read_csv(DATASET_PATH+my_data_set_name,sep=col_sep_char,\r\n","                     encoding = 'unicode_escape', engine ='python',\r\n","                     quoting=3)\r\n","\r\n","  otds.iloc[:,rec_id_col] = otds.iloc[:,rec_id_col].astype(str)\r\n","  myds.iloc[:,rec_id_col] = myds.iloc[:,rec_id_col].astype(str)\r\n","\r\n","  otds['target'] = otds.iloc[:,attr_list].replace(numpy.nan, '', regex=True).agg(' '.join, axis=1)\r\n","  otds['target'] = otds[\"target\"].str.lower()\r\n","\r\n","  ot_ids_list = []\r\n","  multiple_ot_ids_list = []\r\n","\r\n","  for record in records_matched:\r\n","    ot_id = list(otds[otds.target == record].iloc[:,rec_id_col].values)\r\n","    if len(ot_id) == 1:\r\n","      ot_ids_list.append(ot_id[0])\r\n","    else:\r\n","      ## two or more record are identical\r\n","      ## than we consider that for pratical terms the attack is unable to \r\n","      ## reidentify properly\r\n","      multiple_ot_ids_list.append(ot_id)\r\n","\r\n","  num_reidentified_ids = len(ot_ids_list)\r\n","  inter = set(ot_ids_list).intersection(set(myds.iloc[:,rec_id_col].values))\r\n","  num_reidentified_ids_matches = len(inter)\r\n","  num_reidentified_ids_no_matches = num_reidentified_ids - \\\r\n","                                    num_reidentified_ids_matches\r\n","\r\n","  return (num_reidentified_ids,num_reidentified_ids_matches,\r\n","          num_reidentified_ids_no_matches)"],"outputs":[],"metadata":{"id":"58Mg8_Seh1dL","executionInfo":{"status":"ok","timestamp":1630716271869,"user_tz":180,"elapsed":2047,"user":{"displayName":"Thiago Nóbrega","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_-BHXOY4rTl8cwozc96ukJW5fc7kC04rST8UwDgY=s64","userId":"01534443958959954150"}}}},{"cell_type":"markdown","source":["## Main"],"metadata":{"id":"i4dNejE5OSpN"}},{"cell_type":"code","execution_count":null,"source":["#@title Run attack { form-width: \"50%\" }\n","\n","# NUM_MATCHES_TO_TRY = 1000  # or 'all'\n","\n","# initial parameters\n","BF_HASH_FUNCT1 = hashlib.sha1\n","BF_HASH_FUNCT2 = hashlib.md5\n","random.seed(42)\n","\n","DATASET_PATH='/tmp/'\n","try:\n","  RESULTS_PATH\n","except NameError:\n","  RESULTS_PATH='/tmp/'\n","\n","command_line_call = sys.argv\n","\n","\n","#@markdown -----------------\n","#@markdown ##### Dataset:\n","dataset = \"ncvoter-sample\" #@param [\"ncvoter\",\"ncvoter-sample\",\"ncvoter-sample-nm-s2\",\"ncvoter-sample-nm-s3\",\"ncvoter-sample-nm-s4\",\"ncvoter-sample-nm-s5\",\"ncvoter-sample-nm-s6\",\"politicians\",\"politicians-split-2\",\"politicians-split-3\",\"politicians-split-4\",\"politicians-split-5\",\"politicians-split-6\"]\n","\n","if dataset == 'politicians':\n","  my_data_set_name = 'brpoliticians-federal-2014-all-1to1-a.csv.gz'\n","  ot_data_set_name = 'brpoliticians-federal-2018-all-1to1-b.csv.gz'\n","if dataset == 'politicians-split-2':\n","  my_data_set_name = 'brpoliticians-federal-2014-all-1to1-a.csv.gz'\n","  ot_data_set_name = 'brpoliticians-federal-2018-5p-i2.csv.gz'\n","if dataset == 'politicians-split-3':\n","  my_data_set_name = 'brpoliticians-federal-2014-all-1to1-a.csv.gz'\n","  ot_data_set_name = 'brpoliticians-federal-2018-5p-i3.csv.gz'\n","if dataset == 'politicians-split-4':\n","  my_data_set_name = 'brpoliticians-federal-2014-all-1to1-a.csv.gz'\n","  ot_data_set_name = 'brpoliticians-federal-2018-5p-i4.csv.gz'\n","if dataset == 'politicians-split-5':\n","  my_data_set_name = 'brpoliticians-federal-2014-all-1to1-a.csv.gz'\n","  ot_data_set_name = 'brpoliticians-federal-2018-5p-i5.csv.gz'\n","if dataset == 'politicians-split-6':\n","  my_data_set_name = 'brpoliticians-federal-2014-all-1to1-a.csv.gz'\n","  ot_data_set_name = 'brpoliticians-federal-2018-5p-i6.csv.gz'\n","\n","if dataset == 'ncvoter':\n","  my_data_set_name = 'ncvoter-20140619-temporal-balanced-ratio-1to1-a.csv.gz'\n","  ot_data_set_name = 'ncvoter-20140619-temporal-balanced-ratio-1to1-b.csv.gz'\n","if dataset == 'ncvoter-sample':\n","  my_data_set_name = 'ncvoter-20140619-temporal-balanced-ratio-1to1-a.csv.gz'\n","  ot_data_set_name = 'sample-ncvoter-20140619-temporal-balanced-ratio-1to1-b.csv.gz'\n","if dataset == 'ncvoter-sample-nm-s2':\n","  my_data_set_name = 'ncvoter-20140619-temporal-balanced-ratio-1to1-a.csv.gz'\n","  ot_data_set_name = 'sample-ncvoter-20140619-temporal-balanced-ratio-1to1-b-bk_NM-split2.csv.gz'\n","if dataset == 'ncvoter-sample-nm-s3':\n","  my_data_set_name = 'ncvoter-20140619-temporal-balanced-ratio-1to1-a.csv.gz'\n","  ot_data_set_name = 'sample-ncvoter-20140619-temporal-balanced-ratio-1to1-b-bk_NM-split3.csv.gz'\n","if dataset == 'ncvoter-sample-nm-s4':\n","  my_data_set_name = 'ncvoter-20140619-temporal-balanced-ratio-1to1-a.csv.gz'\n","  ot_data_set_name = 'sample-ncvoter-20140619-temporal-balanced-ratio-1to1-b-bk_NM-split4.csv.gz'\n","if dataset == 'ncvoter-sample-nm-s5':\n","  my_data_set_name = 'ncvoter-20140619-temporal-balanced-ratio-1to1-a.csv.gz'\n","  ot_data_set_name = 'sample-ncvoter-20140619-temporal-balanced-ratio-1to1-b-bk_NM-split5.csv.gz'\n","if dataset == 'ncvoter-sample-nm-s6':\n","  my_data_set_name = 'ncvoter-20140619-temporal-balanced-ratio-1to1-a.csv.gz'\n","  ot_data_set_name = 'sample-ncvoter-20140619-temporal-balanced-ratio-1to1-b-bk_NM-split6.csv.gz'\n","\n","rec_id_col   = 0   #@param {type:\"integer\"}\n","\n","#voter_reg_num,name_prefix,first_name,middle_name,last_name,\n","#name_suffix,age,gender,race,ethnic,\n","#street_address,city,state,zip_code,full_phone_num,\n","#birth_place,register_date,download_month\n","#@markdown ###### attr_list parameters\n","#@markdown | ATTRS | POS | ATTRS | POS |\n","#@markdown |:---:|:---:|:---:|:---:|\n","#@markdown |<td colspan=2>NCVR <td colspan=2>BR\n","#@markdown |first_name |3|first_name |1|\n","#@markdown |first_name , last_name|3,5|first_name , last_name|1,2|\n","#@markdown | full_name |3,4,5| full_name |3|\n","#@markdown |first_name , last_name , address|3,5,11|full_name, address|3,9,10|\n","# <sup> NCVR </sup>\n","# > <sup>3,5 - first_name , last_name</sup>\\\n","# > <sup>3,5,11 - first_name , last_name, street_address</sup>\\\n","# > <sup>3,5,6,7,12 - first_name , last_name, age, gender, city</sup>\n","attr_list        =    [3,5]#@param {type:\"raw\"}\n","#@markdown ###### col separator [(',' - ncvr),(';' - brpol)]\n","col_sep_char = ',' #@param {type:\"string\"}[',',';']\n","header_line_flag = True #@param {type:\"raw\"}\n","\n","#@markdown -----------------\n","#@markdown #### Anonymization Parameter:\n","\n","q =  2#@param {type:\"integer\"}\n","hash_type = \"xrh\" #@param [\"rh\",\"xrh\",\"xbf\",\"dh\"]\n","num_hash_funct = \"opt\" #@param [\"opt\", \"opt_half\", \"opt_quarter\"]\n","bf_len =  200#@param {type:\"integer\"}\n","\n","#ajustar depois\n","bf_seg_perc_list = [5] #@param {type:\"raw\"}\n","num_bf_to_attack =  'all'#@param {type:\"raw\"}\n","\n","main_start_time = time.time()\n","\n","# command_line_call = sys.argv\n","# q =                int(sys.argv[1])\n","# hash_type =        sys.argv[2].lower()\n","# num_hash_funct =   sys.argv[3]\n","# bf_len =           int(sys.argv[4])\n","# my_data_set_name = sys.argv[5]\n","# ot_data_set_name = sys.argv[6]\n","# rec_id_col =       int(sys.argv[7])\n","# col_sep_char =     sys.argv[8]\n","# header_line_flag = eval(sys.argv[9])\n","# attr_list =        eval(sys.argv[10])\n","# bf_seg_perc_list = eval(sys.argv[11])\n","# num_bf_to_attack = sys.argv[12]\n","\n","assert q >= 1, q\n","assert hash_type in ['dh','rh','xrh','xbf'], hash_type\n","if num_hash_funct.isdigit():\n","  num_hash_funct = int(num_hash_funct)\n","  assert num_hash_funct >= 1, num_hash_funct\n","else:\n","  assert num_hash_funct in ['opt', 'opt_half', 'opt_quarter'], num_hash_funct\n","assert bf_len > 1, bf_len\n","#\n","assert rec_id_col >= 0, rec_id_col\n","assert header_line_flag in [True,False], header_line_flag\n","assert isinstance(attr_list, list), attr_list\n","assert isinstance(bf_seg_perc_list, list), bf_seg_perc_list\n","if (num_bf_to_attack != 'all'):\n","  num_bf_to_attack = int(num_bf_to_attack)\n","  assert num_bf_to_attack > 1, num_bf_to_attack\n","\n","my_base_data_set_name = my_data_set_name.split('/')[-1].replace('.csv', '')\n","my_base_data_set_name = my_base_data_set_name.replace('.gz','')\n","ot_base_data_set_name = ot_data_set_name.split('/')[-1].replace('.csv', '')\n","ot_base_data_set_name = ot_base_data_set_name.replace('.gz','')\n","\n","assert ',' not in my_base_data_set_name\n","assert ',' not in ot_base_data_set_name\n","\n","print('')\n","print('-'*80)\n","print('')\n","\n","ds_name = my_data_set_name.split('-')[0]\n","res_file = 'real-'+ds_name+'-'+str(num_bf_to_attack)+'-'+str(hash_type)+'-'+str(bf_len)+'.csv'\n","result_file_exists = os.path.exists(RESULTS_PATH + res_file)\n","\n","f = open(RESULTS_PATH + res_file, \"a\")\n","\n","\n","\n","# -----------------------------------------------------------------------------\n","# Step 1: Load the data sets and extract q-grams for selected attributes\n","#\n","my_attr_val_q_gram_dict, my_q_gram_attr_val_dict, my_avr_num_q_gram, \\\n","   my_all_q_gram_set, my_encoded_attr_name_str = \\\n","                       load_data_set_extract_q_grams(my_data_set_name,\n","                                                     rec_id_col, attr_list,\n","                                                     col_sep_char,\n","                                                     header_line_flag, q)\n","\n","ot_attr_val_q_gram_dict, ot_q_gram_attr_val_dict, ot_avr_num_q_gram, \\\n","   ot_all_q_gram_set, ot_encoded_attr_name_str = \\\n","                       load_data_set_extract_q_grams(ot_data_set_name,\n","                                                     rec_id_col, attr_list,\n","                                                     col_sep_char,\n","                                                     header_line_flag, q)\n","\n","assert my_encoded_attr_name_str == ot_encoded_attr_name_str\n","\n","# Generate a dictionary with all q-grams and their attribute values from both\n","# data sets\n","#\n","my_num_q_gram = len(my_q_gram_attr_val_dict)\n","ot_num_q_gram = len(ot_q_gram_attr_val_dict)\n","\n","all_q_gram_attr_val_dict =    {}\n","\n","#modified to python3\n","for (q_gram, attr_val_set) in my_q_gram_attr_val_dict.items():\n","  all_q_gram_attr_val_dict[q_gram] = attr_val_set\n","for (q_gram, attr_val_set) in ot_q_gram_attr_val_dict.items():\n","  q_gram_attr_val_set = all_q_gram_attr_val_dict.get(q_gram, set())\n","  q_gram_attr_val_set = q_gram_attr_val_set | attr_val_set\n","  all_q_gram_attr_val_dict[q_gram] = q_gram_attr_val_set\n","\n","num_all_q_gram =    len(all_q_gram_attr_val_dict)\n","num_common_q_gram = len(set(my_q_gram_attr_val_dict.keys()) & \\\n","                        set(ot_q_gram_attr_val_dict.keys()))\n","\n","print('A total of %d unique q-grams occur in both data set' % (num_all_q_gram))\n","print('  Of these, %d occur in both data sets' % (num_common_q_gram))\n","print('')\n","\n","num_common_attr_val = len(set(my_attr_val_q_gram_dict.keys()) & \\\n","                          set(ot_attr_val_q_gram_dict.keys()))\n","num_my_attr_val = len(my_attr_val_q_gram_dict)\n","num_ot_attr_val = len(ot_attr_val_q_gram_dict)\n","\n","jacc_common_attr_val = float(num_common_attr_val) / \\\n","  (num_my_attr_val + num_ot_attr_val - num_common_attr_val)\n","\n","print('My database contains %d unique attribute values' % (num_my_attr_val))\n","print('The other database contains %d unique attribute values' % \\\n","      (num_ot_attr_val))\n","print('  Number of unique attribute values in common: %d' % \\\n","      (num_common_attr_val))\n","print('    Jaccard based overlap of common attribute values: %.1f%%' % \\\n","      (100.0*jacc_common_attr_val))\n","print('')\n","\n","# -----------------------------------------------------------------------------\n","# Step 2: Generate Bloom filters from q-gram sets\n","#\n","if (num_hash_funct in ['opt', 'opt_half', 'opt_quarter']):\n","\n","  # Set number of hash functions to have in average 50% of bits set to 1\n","  # (Linking Sensitive Data book, 2020)\n","  # num_hash_funct = int(math.ceil(0.5 * BF_LEN / \\\n","  #                                math.floor(avrg_num_q_gram)))\n","  #\n","  opt_num_hash_funct = int(round(numpy.log(2.0) * float(bf_len) / \\\n","                                 my_avr_num_q_gram))\n","  if (num_hash_funct == 'opt'):\n","    num_hash_funct = opt_num_hash_funct\n","  elif (num_hash_funct == 'opt_half'):\n","    num_hash_funct = int(round(opt_num_hash_funct / 2.0))\n","  elif (num_hash_funct == 'opt_quarter'):\n","    num_hash_funct = int(round(opt_num_hash_funct / 4.0))\n","  else:\n","    raise Exception(num_hash_funct)\n","\n","my_bf_dict, my_bit_pos_q_gram_dict, my_atom_bf_dict = \\\n","    gen_bloom_filter_dict(my_attr_val_q_gram_dict, hash_type, bf_len,\n","                          num_hash_funct)\n","\n","\n","ot_bf_dict, ot_bit_pos_q_gram_dict, ot_atom_bf_dict = \\\n","    gen_bloom_filter_dict(ot_attr_val_q_gram_dict, hash_type, bf_len,\n","                          num_hash_funct)\n","\n","#rodando ate aqui\n","\n","# Header string for results output\n","#\n","header = '### command_line_call, encoded_attr_name_str,num_bf_to_attack, ' +\\\n","      'hash_type, num_hash_funct, num_my_attr_val, num_ot_attr_val, ' +\\\n","      'num_common_attr_val, jacc_common_perc, my_num_q_gram, ot_num_q_gram, ' +\\\n","      'num_all_q_gram, num_common_q_gram, bf_seg_len, bf_seg_perc, ' +\\\n","      'my_num_q_gram_bf_seg, my_min_num_bit_pos, my_avr_num_bit_pos, ' +\\\n","      'my_med_num_bit_pos, my_max_num_bit_pos, ot_num_q_gram_bf_seg, ' +\\\n","      'ot_min_num_bit_pos, ot_avr_num_bit_pos, ot_med_num_bit_pos, ' +\\\n","      'ot_max_num_bit_pos, atom_num_corr_1_1_attr_matches, ' +\\\n","      'atom_num_corr_1_m_attr_matches, atom_num_wrong_1_matches, ' +\\\n","      'atom_num_wrong_m_matches, atom_num_no_matches, num_reidentified_ids, ' +\\\n","      'num_reidentified_ids_matches, num_reidentified_ids_no_matches, ' +\\\n","      'reidentified_id_list'\n","\n","print(header)\n","\n","if result_file_exists == False:\n","  f.write(header.split('### ')[1]+'\\n')\n","\n","# -----------------------------------------------------------------------------\n","# Step 3: Run the segment attack with different segment percentages\n","#\n","for bf_seg_perc in bf_seg_perc_list:\n","\n","  print('-'*80)\n","  print()\n","\n","  # Generate a string to be printed as the result summary\n","  #\n","  res_str = '### \"' + ' '.join(command_line_call)\n","  res_str = res_str.replace(' , ', ' comma ')  # The separator in input file\n","  res_str = res_str.replace(',', ';') + '\", '\n","\n","  # Details of how many attribute values in the two data sets, and how many\n","  # occur in common in both data sets\n","  #\n","  res_str += '%s, %s, %s, %d, %d, %d, %d, %.1f, ' % (my_encoded_attr_name_str,\n","                                                str(num_bf_to_attack),\n","                                                hash_type,\n","                                                num_hash_funct, num_my_attr_val,\n","                                                num_ot_attr_val,\n","                                                num_common_attr_val,\n","                                                100.0*jacc_common_attr_val)\n","\n","  # Details of how many q-grams in the two data sets, how many in total,\n","  # and how many occur occur in common in both data sets\n","  #\n","  res_str += '%d, %d, %d, %d, ' % (my_num_q_gram, ot_num_q_gram, \\\n","                                   num_all_q_gram, num_common_q_gram)\n","\n","  if hash_type == 'xbf':\n","    bf_seg_len = int((float(bf_len)/2)*bf_seg_perc/100)\n","  else:\n","    bf_seg_len = int(float(bf_len)*bf_seg_perc/100)\n","\n","  # Get the BF segments from the BFs of both data sets\n","  #\n","  my_bf_seg_dict = get_bf_segments(my_bf_dict, bf_seg_len)\n","  ot_bf_seg_dict = get_bf_segments(ot_bf_dict, bf_seg_len)\n","  \n","  my_atom_bf_seg_dict = get_bf_segments(my_atom_bf_dict, bf_seg_len)\n","  \n","  my_num_q_gram_bf_seg, my_min_num_bit_pos, my_avr_num_bit_pos, \\\n","         my_med_num_bit_pos, my_max_num_bit_pos = \\\n","                  bf_segment_get_num_q_gram(my_bit_pos_q_gram_dict,\n","                                            bf_seg_len, num_hash_funct)\n","  ot_num_q_gram_bf_seg, ot_min_num_bit_pos, ot_avr_num_bit_pos, \\\n","         ot_med_num_bit_pos, ot_max_num_bit_pos = \\\n","                  bf_segment_get_num_q_gram(ot_bit_pos_q_gram_dict,\n","                                            bf_seg_len, num_hash_funct)\n","\n","  # Add BF segment information to result string\n","  #\n","  res_str += '%d, %d%%, %d, %d, %.2f, %d, %d, ' % (bf_seg_len, bf_seg_perc, \\\n","                                                 my_num_q_gram_bf_seg, \\\n","                                                 my_min_num_bit_pos, \\\n","                                                 my_avr_num_bit_pos, \\\n","                                                 my_med_num_bit_pos, \\\n","                                                 my_max_num_bit_pos)\n","  res_str += '%d, %d, %.2f, %d, %d, ' % (ot_num_q_gram_bf_seg, \\\n","                                       ot_min_num_bit_pos, \\\n","                                       ot_avr_num_bit_pos, \\\n","                                       ot_med_num_bit_pos, \\\n","                                       ot_max_num_bit_pos)\n","\n","  # Atom based attack\n","  #\n","  num_corr_1_1_attr_matches, num_corr_1_m_attr_matches, \\\n","      num_wrong_1_matches, num_wrong_m_matches, \\\n","      num_no_matches,records_matched = bf_segment_atom_attack(ot_bf_seg_dict,\n","                                              my_atom_bf_seg_dict,\n","                                              my_bit_pos_q_gram_dict,\n","                                              all_q_gram_attr_val_dict,\n","                                              num_bf_to_attack)\n","\n","  res_str += '%d, %d, %d, %d, %d, ' % (num_corr_1_1_attr_matches, \\\n","                                     num_corr_1_m_attr_matches, \\\n","                                     num_wrong_1_matches, num_wrong_m_matches, \\\n","                                     num_no_matches)\n","\n","  # profile \n","  #\n","  num_reidentified_ids,num_reidentified_ids_matches, \\\n","  num_reidentified_ids_no_matches = profile_reidentied_records(DATASET_PATH,\n","                                          my_data_set_name,ot_data_set_name,\n","                                          col_sep_char,rec_id_col, attr_list)\n","  \n","  res_str += '%d, %d, %d,' % (num_reidentified_ids,num_reidentified_ids_matches,\n","                              num_reidentified_ids_no_matches)\n","  \n","  saida_lista = ''\n","  for r_id in records_matched:\n","    saida_lista += str(r_id)+'#'\n","\n","  res_str += '%s' % (saida_lista) \n","  # Print result line for CSV generation\n","  #\n","\n","  print(res_str)\n","  f.write(res_str.split('### ')[1]+'\\n') #savinf csv file\n","  f.flush()\n","\n","f.close()\n"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fkx6eoDImaj3","cellView":"form","executionInfo":{"status":"ok","timestamp":1630731278475,"user_tz":180,"elapsed":15000543,"user":{"displayName":"Thiago Nóbrega","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_-BHXOY4rTl8cwozc96ukJW5fc7kC04rST8UwDgY=s64","userId":"01534443958959954150"}},"outputId":"c209266c-d4d2-40a8-f5ec-b8cbc8c39a87"}},{"cell_type":"code","execution_count":null,"source":["f.close()"],"outputs":[],"metadata":{"id":"ulfOoIfdsAqG"}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{"id":"VNiSLgAHKFfX"}},{"cell_type":"code","execution_count":null,"source":["#@title Run attack { form-width: \"50%\" }\n","\n","# NUM_MATCHES_TO_TRY = 1000  # or 'all'\n","\n","# initial parameters\n","BF_HASH_FUNCT1 = hashlib.sha1\n","BF_HASH_FUNCT2 = hashlib.md5\n","random.seed(42)\n","\n","DATASET_PATH='/tmp/'\n","try:\n","  RESULTS_PATH\n","except NameError:\n","  RESULTS_PATH='/tmp/'\n","\n","command_line_call = sys.argv\n","\n","\n","#@markdown -----------------\n","#@markdown ##### Dataset:\n","dataset = \"ncvoter-sample-nm-s2\" #@param [\"ncvoter\",\"ncvoter-sample\",\"ncvoter-sample-nm-s2\",\"ncvoter-sample-nm-s3\",\"ncvoter-sample-nm-s4\",\"ncvoter-sample-nm-s5\",\"ncvoter-sample-nm-s6\",\"politicians\",\"politicians-split-2\",\"politicians-split-3\",\"politicians-split-4\",\"politicians-split-5\",\"politicians-split-6\"]\n","\n","if dataset == 'politicians':\n","  my_data_set_name = 'brpoliticians-federal-2014-all-1to1-a.csv.gz'\n","  ot_data_set_name = 'brpoliticians-federal-2018-all-1to1-b.csv.gz'\n","if dataset == 'politicians-split-2':\n","  my_data_set_name = 'brpoliticians-federal-2014-all-1to1-a.csv.gz'\n","  ot_data_set_name = 'brpoliticians-federal-2018-5p-i2.csv.gz'\n","if dataset == 'politicians-split-3':\n","  my_data_set_name = 'brpoliticians-federal-2014-all-1to1-a.csv.gz'\n","  ot_data_set_name = 'brpoliticians-federal-2018-5p-i3.csv.gz'\n","if dataset == 'politicians-split-4':\n","  my_data_set_name = 'brpoliticians-federal-2014-all-1to1-a.csv.gz'\n","  ot_data_set_name = 'brpoliticians-federal-2018-5p-i4.csv.gz'\n","if dataset == 'politicians-split-5':\n","  my_data_set_name = 'brpoliticians-federal-2014-all-1to1-a.csv.gz'\n","  ot_data_set_name = 'brpoliticians-federal-2018-5p-i5.csv.gz'\n","if dataset == 'politicians-split-6':\n","  my_data_set_name = 'brpoliticians-federal-2014-all-1to1-a.csv.gz'\n","  ot_data_set_name = 'brpoliticians-federal-2018-5p-i6.csv.gz'\n","\n","if dataset == 'ncvoter':\n","  my_data_set_name = 'ncvoter-20140619-temporal-balanced-ratio-1to1-a.csv.gz'\n","  ot_data_set_name = 'ncvoter-20140619-temporal-balanced-ratio-1to1-b.csv.gz'\n","if dataset == 'ncvoter-sample':\n","  my_data_set_name = 'ncvoter-20140619-temporal-balanced-ratio-1to1-a.csv.gz'\n","  ot_data_set_name = 'sample-ncvoter-20140619-temporal-balanced-ratio-1to1-b.csv.gz'\n","if dataset == 'ncvoter-sample-nm-s2':\n","  my_data_set_name = 'ncvoter-20140619-temporal-balanced-ratio-1to1-a.csv.gz'\n","  ot_data_set_name = 'sample-ncvoter-20140619-temporal-balanced-ratio-1to1-b-bk_NM-split2.csv.gz'\n","if dataset == 'ncvoter-sample-nm-s3':\n","  my_data_set_name = 'ncvoter-20140619-temporal-balanced-ratio-1to1-a.csv.gz'\n","  ot_data_set_name = 'sample-ncvoter-20140619-temporal-balanced-ratio-1to1-b-bk_NM-split3.csv.gz'\n","if dataset == 'ncvoter-sample-nm-s4':\n","  my_data_set_name = 'ncvoter-20140619-temporal-balanced-ratio-1to1-a.csv.gz'\n","  ot_data_set_name = 'sample-ncvoter-20140619-temporal-balanced-ratio-1to1-b-bk_NM-split4.csv.gz'\n","if dataset == 'ncvoter-sample-nm-s5':\n","  my_data_set_name = 'ncvoter-20140619-temporal-balanced-ratio-1to1-a.csv.gz'\n","  ot_data_set_name = 'sample-ncvoter-20140619-temporal-balanced-ratio-1to1-b-bk_NM-split5.csv.gz'\n","if dataset == 'ncvoter-sample-nm-s6':\n","  my_data_set_name = 'ncvoter-20140619-temporal-balanced-ratio-1to1-a.csv.gz'\n","  ot_data_set_name = 'sample-ncvoter-20140619-temporal-balanced-ratio-1to1-b-bk_NM-split6.csv.gz'\n","\n","rec_id_col   = 0   #@param {type:\"integer\"}\n","\n","#voter_reg_num,name_prefix,first_name,middle_name,last_name,\n","#name_suffix,age,gender,race,ethnic,\n","#street_address,city,state,zip_code,full_phone_num,\n","#birth_place,register_date,download_month\n","#@markdown ###### attr_list parameters\n","#@markdown | ATTRS | POS | ATTRS | POS |\n","#@markdown |:---:|:---:|:---:|:---:|\n","#@markdown |<td colspan=2>NCVR <td colspan=2>BR\n","#@markdown |first_name |3|first_name |1|\n","#@markdown |first_name , last_name|3,5|first_name , last_name|1,2|\n","#@markdown | full_name |3,4,5| full_name |3|\n","#@markdown |first_name , last_name , address|3,5,11|full_name, address|3,9,10|\n","# <sup> NCVR </sup>\n","# > <sup>3,5 - first_name , last_name</sup>\\\n","# > <sup>3,5,11 - first_name , last_name, street_address</sup>\\\n","# > <sup>3,5,6,7,12 - first_name , last_name, age, gender, city</sup>\n","attr_list        =    [3,5]#@param {type:\"raw\"}\n","#@markdown ###### col separator [(',' - ncvr),(';' - brpol)]\n","col_sep_char = ',' #@param {type:\"string\"}[',',';']\n","header_line_flag = True #@param {type:\"raw\"}\n","\n","#@markdown -----------------\n","#@markdown #### Anonymization Parameter:\n","\n","q =  2#@param {type:\"integer\"}\n","hash_type = \"xrh\" #@param [\"rh\",\"xrh\",\"xbf\",\"dh\"]\n","num_hash_funct = \"opt\" #@param [\"opt\", \"opt_half\", \"opt_quarter\"]\n","bf_len =  200#@param {type:\"integer\"}\n","\n","#ajustar depois\n","bf_seg_perc_list = [10] #@param {type:\"raw\"}\n","num_bf_to_attack =  'all'#@param {type:\"raw\"}\n","\n","main_start_time = time.time()\n","\n","# command_line_call = sys.argv\n","# q =                int(sys.argv[1])\n","# hash_type =        sys.argv[2].lower()\n","# num_hash_funct =   sys.argv[3]\n","# bf_len =           int(sys.argv[4])\n","# my_data_set_name = sys.argv[5]\n","# ot_data_set_name = sys.argv[6]\n","# rec_id_col =       int(sys.argv[7])\n","# col_sep_char =     sys.argv[8]\n","# header_line_flag = eval(sys.argv[9])\n","# attr_list =        eval(sys.argv[10])\n","# bf_seg_perc_list = eval(sys.argv[11])\n","# num_bf_to_attack = sys.argv[12]\n","\n","assert q >= 1, q\n","assert hash_type in ['dh','rh','xrh','xbf'], hash_type\n","if num_hash_funct.isdigit():\n","  num_hash_funct = int(num_hash_funct)\n","  assert num_hash_funct >= 1, num_hash_funct\n","else:\n","  assert num_hash_funct in ['opt', 'opt_half', 'opt_quarter'], num_hash_funct\n","assert bf_len > 1, bf_len\n","#\n","assert rec_id_col >= 0, rec_id_col\n","assert header_line_flag in [True,False], header_line_flag\n","assert isinstance(attr_list, list), attr_list\n","assert isinstance(bf_seg_perc_list, list), bf_seg_perc_list\n","if (num_bf_to_attack != 'all'):\n","  num_bf_to_attack = int(num_bf_to_attack)\n","  assert num_bf_to_attack > 1, num_bf_to_attack\n","\n","my_base_data_set_name = my_data_set_name.split('/')[-1].replace('.csv', '')\n","my_base_data_set_name = my_base_data_set_name.replace('.gz','')\n","ot_base_data_set_name = ot_data_set_name.split('/')[-1].replace('.csv', '')\n","ot_base_data_set_name = ot_base_data_set_name.replace('.gz','')\n","\n","assert ',' not in my_base_data_set_name\n","assert ',' not in ot_base_data_set_name\n","\n","print('')\n","print('-'*80)\n","print('')\n","\n","ds_name = my_data_set_name.split('-')[0]\n","res_file = 'real-'+ds_name+'-'+str(num_bf_to_attack)+'-'+str(hash_type)+'-'+str(bf_len)+'.csv'\n","result_file_exists = os.path.exists(RESULTS_PATH + res_file)\n","\n","f = open(RESULTS_PATH + res_file, \"a\")\n","\n","\n","\n","# -----------------------------------------------------------------------------\n","# Step 1: Load the data sets and extract q-grams for selected attributes\n","#\n","my_attr_val_q_gram_dict, my_q_gram_attr_val_dict, my_avr_num_q_gram, \\\n","   my_all_q_gram_set, my_encoded_attr_name_str = \\\n","                       load_data_set_extract_q_grams(my_data_set_name,\n","                                                     rec_id_col, attr_list,\n","                                                     col_sep_char,\n","                                                     header_line_flag, q)\n","\n","ot_attr_val_q_gram_dict, ot_q_gram_attr_val_dict, ot_avr_num_q_gram, \\\n","   ot_all_q_gram_set, ot_encoded_attr_name_str = \\\n","                       load_data_set_extract_q_grams(ot_data_set_name,\n","                                                     rec_id_col, attr_list,\n","                                                     col_sep_char,\n","                                                     header_line_flag, q)\n","\n","assert my_encoded_attr_name_str == ot_encoded_attr_name_str\n","\n","# Generate a dictionary with all q-grams and their attribute values from both\n","# data sets\n","#\n","my_num_q_gram = len(my_q_gram_attr_val_dict)\n","ot_num_q_gram = len(ot_q_gram_attr_val_dict)\n","\n","all_q_gram_attr_val_dict =    {}\n","\n","#modified to python3\n","for (q_gram, attr_val_set) in my_q_gram_attr_val_dict.items():\n","  all_q_gram_attr_val_dict[q_gram] = attr_val_set\n","for (q_gram, attr_val_set) in ot_q_gram_attr_val_dict.items():\n","  q_gram_attr_val_set = all_q_gram_attr_val_dict.get(q_gram, set())\n","  q_gram_attr_val_set = q_gram_attr_val_set | attr_val_set\n","  all_q_gram_attr_val_dict[q_gram] = q_gram_attr_val_set\n","\n","num_all_q_gram =    len(all_q_gram_attr_val_dict)\n","num_common_q_gram = len(set(my_q_gram_attr_val_dict.keys()) & \\\n","                        set(ot_q_gram_attr_val_dict.keys()))\n","\n","print('A total of %d unique q-grams occur in both data set' % (num_all_q_gram))\n","print('  Of these, %d occur in both data sets' % (num_common_q_gram))\n","print('')\n","\n","num_common_attr_val = len(set(my_attr_val_q_gram_dict.keys()) & \\\n","                          set(ot_attr_val_q_gram_dict.keys()))\n","num_my_attr_val = len(my_attr_val_q_gram_dict)\n","num_ot_attr_val = len(ot_attr_val_q_gram_dict)\n","\n","jacc_common_attr_val = float(num_common_attr_val) / \\\n","  (num_my_attr_val + num_ot_attr_val - num_common_attr_val)\n","\n","print('My database contains %d unique attribute values' % (num_my_attr_val))\n","print('The other database contains %d unique attribute values' % \\\n","      (num_ot_attr_val))\n","print('  Number of unique attribute values in common: %d' % \\\n","      (num_common_attr_val))\n","print('    Jaccard based overlap of common attribute values: %.1f%%' % \\\n","      (100.0*jacc_common_attr_val))\n","print('')\n","\n","# -----------------------------------------------------------------------------\n","# Step 2: Generate Bloom filters from q-gram sets\n","#\n","if (num_hash_funct in ['opt', 'opt_half', 'opt_quarter']):\n","\n","  # Set number of hash functions to have in average 50% of bits set to 1\n","  # (Linking Sensitive Data book, 2020)\n","  # num_hash_funct = int(math.ceil(0.5 * BF_LEN / \\\n","  #                                math.floor(avrg_num_q_gram)))\n","  #\n","  opt_num_hash_funct = int(round(numpy.log(2.0) * float(bf_len) / \\\n","                                 my_avr_num_q_gram))\n","  if (num_hash_funct == 'opt'):\n","    num_hash_funct = opt_num_hash_funct\n","  elif (num_hash_funct == 'opt_half'):\n","    num_hash_funct = int(round(opt_num_hash_funct / 2.0))\n","  elif (num_hash_funct == 'opt_quarter'):\n","    num_hash_funct = int(round(opt_num_hash_funct / 4.0))\n","  else:\n","    raise Exception(num_hash_funct)\n","\n","my_bf_dict, my_bit_pos_q_gram_dict, my_atom_bf_dict = \\\n","    gen_bloom_filter_dict(my_attr_val_q_gram_dict, hash_type, bf_len,\n","                          num_hash_funct)\n","\n","\n","ot_bf_dict, ot_bit_pos_q_gram_dict, ot_atom_bf_dict = \\\n","    gen_bloom_filter_dict(ot_attr_val_q_gram_dict, hash_type, bf_len,\n","                          num_hash_funct)\n","\n","#rodando ate aqui\n","\n","# Header string for results output\n","#\n","header = '### command_line_call, encoded_attr_name_str,num_bf_to_attack, ' +\\\n","      'hash_type, num_hash_funct, num_my_attr_val, num_ot_attr_val, ' +\\\n","      'num_common_attr_val, jacc_common_perc, my_num_q_gram, ot_num_q_gram, ' +\\\n","      'num_all_q_gram, num_common_q_gram, bf_seg_len, bf_seg_perc, ' +\\\n","      'my_num_q_gram_bf_seg, my_min_num_bit_pos, my_avr_num_bit_pos, ' +\\\n","      'my_med_num_bit_pos, my_max_num_bit_pos, ot_num_q_gram_bf_seg, ' +\\\n","      'ot_min_num_bit_pos, ot_avr_num_bit_pos, ot_med_num_bit_pos, ' +\\\n","      'ot_max_num_bit_pos, atom_num_corr_1_1_attr_matches, ' +\\\n","      'atom_num_corr_1_m_attr_matches, atom_num_wrong_1_matches, ' +\\\n","      'atom_num_wrong_m_matches, atom_num_no_matches, num_reidentified_ids, ' +\\\n","      'num_reidentified_ids_matches, num_reidentified_ids_no_matches, ' +\\\n","      'reidentified_id_list'\n","\n","print(header)\n","\n","if result_file_exists == False:\n","  f.write(header.split('### ')[1]+'\\n')\n","\n","# -----------------------------------------------------------------------------\n","# Step 3: Run the segment attack with different segment percentages\n","#\n","for bf_seg_perc in bf_seg_perc_list:\n","\n","  print('-'*80)\n","  print()\n","\n","  # Generate a string to be printed as the result summary\n","  #\n","  res_str = '### \"' + ' '.join(command_line_call)\n","  res_str = res_str.replace(' , ', ' comma ')  # The separator in input file\n","  res_str = res_str.replace(',', ';') + '\", '\n","\n","  # Details of how many attribute values in the two data sets, and how many\n","  # occur in common in both data sets\n","  #\n","  res_str += '%s, %s, %s, %d, %d, %d, %d, %.1f, ' % (my_encoded_attr_name_str,\n","                                                str(num_bf_to_attack),\n","                                                hash_type,\n","                                                num_hash_funct, num_my_attr_val,\n","                                                num_ot_attr_val,\n","                                                num_common_attr_val,\n","                                                100.0*jacc_common_attr_val)\n","\n","  # Details of how many q-grams in the two data sets, how many in total,\n","  # and how many occur occur in common in both data sets\n","  #\n","  res_str += '%d, %d, %d, %d, ' % (my_num_q_gram, ot_num_q_gram, \\\n","                                   num_all_q_gram, num_common_q_gram)\n","\n","  if hash_type == 'xbf':\n","    bf_seg_len = int((float(bf_len)/2)*bf_seg_perc/100)\n","  else:\n","    bf_seg_len = int(float(bf_len)*bf_seg_perc/100)\n","\n","  # Get the BF segments from the BFs of both data sets\n","  #\n","  my_bf_seg_dict = get_bf_segments(my_bf_dict, bf_seg_len)\n","  ot_bf_seg_dict = get_bf_segments(ot_bf_dict, bf_seg_len)\n","  \n","  my_atom_bf_seg_dict = get_bf_segments(my_atom_bf_dict, bf_seg_len)\n","  \n","  my_num_q_gram_bf_seg, my_min_num_bit_pos, my_avr_num_bit_pos, \\\n","         my_med_num_bit_pos, my_max_num_bit_pos = \\\n","                  bf_segment_get_num_q_gram(my_bit_pos_q_gram_dict,\n","                                            bf_seg_len, num_hash_funct)\n","  ot_num_q_gram_bf_seg, ot_min_num_bit_pos, ot_avr_num_bit_pos, \\\n","         ot_med_num_bit_pos, ot_max_num_bit_pos = \\\n","                  bf_segment_get_num_q_gram(ot_bit_pos_q_gram_dict,\n","                                            bf_seg_len, num_hash_funct)\n","\n","  # Add BF segment information to result string\n","  #\n","  res_str += '%d, %d%%, %d, %d, %.2f, %d, %d, ' % (bf_seg_len, bf_seg_perc, \\\n","                                                 my_num_q_gram_bf_seg, \\\n","                                                 my_min_num_bit_pos, \\\n","                                                 my_avr_num_bit_pos, \\\n","                                                 my_med_num_bit_pos, \\\n","                                                 my_max_num_bit_pos)\n","  res_str += '%d, %d, %.2f, %d, %d, ' % (ot_num_q_gram_bf_seg, \\\n","                                       ot_min_num_bit_pos, \\\n","                                       ot_avr_num_bit_pos, \\\n","                                       ot_med_num_bit_pos, \\\n","                                       ot_max_num_bit_pos)\n","\n","  # Atom based attack\n","  #\n","  num_corr_1_1_attr_matches, num_corr_1_m_attr_matches, \\\n","      num_wrong_1_matches, num_wrong_m_matches, \\\n","      num_no_matches,records_matched = bf_segment_atom_attack(ot_bf_seg_dict,\n","                                              my_atom_bf_seg_dict,\n","                                              my_bit_pos_q_gram_dict,\n","                                              all_q_gram_attr_val_dict,\n","                                              num_bf_to_attack)\n","\n","  res_str += '%d, %d, %d, %d, %d, ' % (num_corr_1_1_attr_matches, \\\n","                                     num_corr_1_m_attr_matches, \\\n","                                     num_wrong_1_matches, num_wrong_m_matches, \\\n","                                     num_no_matches)\n","\n","  # profile \n","  #\n","  num_reidentified_ids,num_reidentified_ids_matches, \\\n","  num_reidentified_ids_no_matches = profile_reidentied_records(DATASET_PATH,\n","                                          my_data_set_name,ot_data_set_name,\n","                                          col_sep_char,rec_id_col, attr_list)\n","  \n","  res_str += '%d, %d, %d,' % (num_reidentified_ids,num_reidentified_ids_matches,\n","                              num_reidentified_ids_no_matches)\n","  \n","  saida_lista = ''\n","  for r_id in records_matched:\n","    saida_lista += str(r_id)+'#'\n","\n","  res_str += '%s' % (saida_lista) \n","  # Print result line for CSV generation\n","  #\n","\n","  print(res_str)\n","  f.write(res_str.split('### ')[1]+'\\n') #savinf csv file\n","  f.flush()\n","\n","f.close()\n"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"P_nK3apIQ9bh","executionInfo":{"status":"ok","timestamp":1630735501717,"user_tz":180,"elapsed":4223247,"user":{"displayName":"Thiago Nóbrega","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_-BHXOY4rTl8cwozc96ukJW5fc7kC04rST8UwDgY=s64","userId":"01534443958959954150"}},"outputId":"50eed594-0f69-4ef1-ae03-b0a8d0e1602c"}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{"id":"ZlLnE9MmRBB1"}},{"cell_type":"code","execution_count":null,"source":["#@title Run attack { form-width: \"50%\" }\n","\n","# NUM_MATCHES_TO_TRY = 1000  # or 'all'\n","\n","# initial parameters\n","BF_HASH_FUNCT1 = hashlib.sha1\n","BF_HASH_FUNCT2 = hashlib.md5\n","random.seed(42)\n","\n","DATASET_PATH='/tmp/'\n","try:\n","  RESULTS_PATH\n","except NameError:\n","  RESULTS_PATH='/tmp/'\n","\n","command_line_call = sys.argv\n","\n","\n","#@markdown -----------------\n","#@markdown ##### Dataset:\n","dataset = \"ncvoter-sample-nm-s3\" #@param [\"ncvoter\",\"ncvoter-sample\",\"ncvoter-sample-nm-s2\",\"ncvoter-sample-nm-s3\",\"ncvoter-sample-nm-s4\",\"ncvoter-sample-nm-s5\",\"ncvoter-sample-nm-s6\",\"politicians\",\"politicians-split-2\",\"politicians-split-3\",\"politicians-split-4\",\"politicians-split-5\",\"politicians-split-6\"]\n","\n","if dataset == 'politicians':\n","  my_data_set_name = 'brpoliticians-federal-2014-all-1to1-a.csv.gz'\n","  ot_data_set_name = 'brpoliticians-federal-2018-all-1to1-b.csv.gz'\n","if dataset == 'politicians-split-2':\n","  my_data_set_name = 'brpoliticians-federal-2014-all-1to1-a.csv.gz'\n","  ot_data_set_name = 'brpoliticians-federal-2018-5p-i2.csv.gz'\n","if dataset == 'politicians-split-3':\n","  my_data_set_name = 'brpoliticians-federal-2014-all-1to1-a.csv.gz'\n","  ot_data_set_name = 'brpoliticians-federal-2018-5p-i3.csv.gz'\n","if dataset == 'politicians-split-4':\n","  my_data_set_name = 'brpoliticians-federal-2014-all-1to1-a.csv.gz'\n","  ot_data_set_name = 'brpoliticians-federal-2018-5p-i4.csv.gz'\n","if dataset == 'politicians-split-5':\n","  my_data_set_name = 'brpoliticians-federal-2014-all-1to1-a.csv.gz'\n","  ot_data_set_name = 'brpoliticians-federal-2018-5p-i5.csv.gz'\n","if dataset == 'politicians-split-6':\n","  my_data_set_name = 'brpoliticians-federal-2014-all-1to1-a.csv.gz'\n","  ot_data_set_name = 'brpoliticians-federal-2018-5p-i6.csv.gz'\n","\n","if dataset == 'ncvoter':\n","  my_data_set_name = 'ncvoter-20140619-temporal-balanced-ratio-1to1-a.csv.gz'\n","  ot_data_set_name = 'ncvoter-20140619-temporal-balanced-ratio-1to1-b.csv.gz'\n","if dataset == 'ncvoter-sample':\n","  my_data_set_name = 'ncvoter-20140619-temporal-balanced-ratio-1to1-a.csv.gz'\n","  ot_data_set_name = 'sample-ncvoter-20140619-temporal-balanced-ratio-1to1-b.csv.gz'\n","if dataset == 'ncvoter-sample-nm-s2':\n","  my_data_set_name = 'ncvoter-20140619-temporal-balanced-ratio-1to1-a.csv.gz'\n","  ot_data_set_name = 'sample-ncvoter-20140619-temporal-balanced-ratio-1to1-b-bk_NM-split2.csv.gz'\n","if dataset == 'ncvoter-sample-nm-s3':\n","  my_data_set_name = 'ncvoter-20140619-temporal-balanced-ratio-1to1-a.csv.gz'\n","  ot_data_set_name = 'sample-ncvoter-20140619-temporal-balanced-ratio-1to1-b-bk_NM-split3.csv.gz'\n","if dataset == 'ncvoter-sample-nm-s4':\n","  my_data_set_name = 'ncvoter-20140619-temporal-balanced-ratio-1to1-a.csv.gz'\n","  ot_data_set_name = 'sample-ncvoter-20140619-temporal-balanced-ratio-1to1-b-bk_NM-split4.csv.gz'\n","if dataset == 'ncvoter-sample-nm-s5':\n","  my_data_set_name = 'ncvoter-20140619-temporal-balanced-ratio-1to1-a.csv.gz'\n","  ot_data_set_name = 'sample-ncvoter-20140619-temporal-balanced-ratio-1to1-b-bk_NM-split5.csv.gz'\n","if dataset == 'ncvoter-sample-nm-s6':\n","  my_data_set_name = 'ncvoter-20140619-temporal-balanced-ratio-1to1-a.csv.gz'\n","  ot_data_set_name = 'sample-ncvoter-20140619-temporal-balanced-ratio-1to1-b-bk_NM-split6.csv.gz'\n","\n","rec_id_col   = 0   #@param {type:\"integer\"}\n","\n","#voter_reg_num,name_prefix,first_name,middle_name,last_name,\n","#name_suffix,age,gender,race,ethnic,\n","#street_address,city,state,zip_code,full_phone_num,\n","#birth_place,register_date,download_month\n","#@markdown ###### attr_list parameters\n","#@markdown | ATTRS | POS | ATTRS | POS |\n","#@markdown |:---:|:---:|:---:|:---:|\n","#@markdown |<td colspan=2>NCVR <td colspan=2>BR\n","#@markdown |first_name |3|first_name |1|\n","#@markdown |first_name , last_name|3,5|first_name , last_name|1,2|\n","#@markdown | full_name |3,4,5| full_name |3|\n","#@markdown |first_name , last_name , address|3,5,11|full_name, address|3,9,10|\n","# <sup> NCVR </sup>\n","# > <sup>3,5 - first_name , last_name</sup>\\\n","# > <sup>3,5,11 - first_name , last_name, street_address</sup>\\\n","# > <sup>3,5,6,7,12 - first_name , last_name, age, gender, city</sup>\n","attr_list        =    [3,5]#@param {type:\"raw\"}\n","#@markdown ###### col separator [(',' - ncvr),(';' - brpol)]\n","col_sep_char = ',' #@param {type:\"string\"}[',',';']\n","header_line_flag = True #@param {type:\"raw\"}\n","\n","#@markdown -----------------\n","#@markdown #### Anonymization Parameter:\n","\n","q =  2#@param {type:\"integer\"}\n","hash_type = \"xrh\" #@param [\"rh\",\"xrh\",\"xbf\",\"dh\"]\n","num_hash_funct = \"opt\" #@param [\"opt\", \"opt_half\", \"opt_quarter\"]\n","bf_len =  200#@param {type:\"integer\"}\n","\n","#ajustar depois\n","bf_seg_perc_list = [15] #@param {type:\"raw\"}\n","num_bf_to_attack =  'all'#@param {type:\"raw\"}\n","\n","main_start_time = time.time()\n","\n","# command_line_call = sys.argv\n","# q =                int(sys.argv[1])\n","# hash_type =        sys.argv[2].lower()\n","# num_hash_funct =   sys.argv[3]\n","# bf_len =           int(sys.argv[4])\n","# my_data_set_name = sys.argv[5]\n","# ot_data_set_name = sys.argv[6]\n","# rec_id_col =       int(sys.argv[7])\n","# col_sep_char =     sys.argv[8]\n","# header_line_flag = eval(sys.argv[9])\n","# attr_list =        eval(sys.argv[10])\n","# bf_seg_perc_list = eval(sys.argv[11])\n","# num_bf_to_attack = sys.argv[12]\n","\n","assert q >= 1, q\n","assert hash_type in ['dh','rh','xrh','xbf'], hash_type\n","if num_hash_funct.isdigit():\n","  num_hash_funct = int(num_hash_funct)\n","  assert num_hash_funct >= 1, num_hash_funct\n","else:\n","  assert num_hash_funct in ['opt', 'opt_half', 'opt_quarter'], num_hash_funct\n","assert bf_len > 1, bf_len\n","#\n","assert rec_id_col >= 0, rec_id_col\n","assert header_line_flag in [True,False], header_line_flag\n","assert isinstance(attr_list, list), attr_list\n","assert isinstance(bf_seg_perc_list, list), bf_seg_perc_list\n","if (num_bf_to_attack != 'all'):\n","  num_bf_to_attack = int(num_bf_to_attack)\n","  assert num_bf_to_attack > 1, num_bf_to_attack\n","\n","my_base_data_set_name = my_data_set_name.split('/')[-1].replace('.csv', '')\n","my_base_data_set_name = my_base_data_set_name.replace('.gz','')\n","ot_base_data_set_name = ot_data_set_name.split('/')[-1].replace('.csv', '')\n","ot_base_data_set_name = ot_base_data_set_name.replace('.gz','')\n","\n","assert ',' not in my_base_data_set_name\n","assert ',' not in ot_base_data_set_name\n","\n","print('')\n","print('-'*80)\n","print('')\n","\n","ds_name = my_data_set_name.split('-')[0]\n","res_file = 'real-'+ds_name+'-'+str(num_bf_to_attack)+'-'+str(hash_type)+'-'+str(bf_len)+'.csv'\n","result_file_exists = os.path.exists(RESULTS_PATH + res_file)\n","\n","f = open(RESULTS_PATH + res_file, \"a\")\n","\n","\n","\n","# -----------------------------------------------------------------------------\n","# Step 1: Load the data sets and extract q-grams for selected attributes\n","#\n","my_attr_val_q_gram_dict, my_q_gram_attr_val_dict, my_avr_num_q_gram, \\\n","   my_all_q_gram_set, my_encoded_attr_name_str = \\\n","                       load_data_set_extract_q_grams(my_data_set_name,\n","                                                     rec_id_col, attr_list,\n","                                                     col_sep_char,\n","                                                     header_line_flag, q)\n","\n","ot_attr_val_q_gram_dict, ot_q_gram_attr_val_dict, ot_avr_num_q_gram, \\\n","   ot_all_q_gram_set, ot_encoded_attr_name_str = \\\n","                       load_data_set_extract_q_grams(ot_data_set_name,\n","                                                     rec_id_col, attr_list,\n","                                                     col_sep_char,\n","                                                     header_line_flag, q)\n","\n","assert my_encoded_attr_name_str == ot_encoded_attr_name_str\n","\n","# Generate a dictionary with all q-grams and their attribute values from both\n","# data sets\n","#\n","my_num_q_gram = len(my_q_gram_attr_val_dict)\n","ot_num_q_gram = len(ot_q_gram_attr_val_dict)\n","\n","all_q_gram_attr_val_dict =    {}\n","\n","#modified to python3\n","for (q_gram, attr_val_set) in my_q_gram_attr_val_dict.items():\n","  all_q_gram_attr_val_dict[q_gram] = attr_val_set\n","for (q_gram, attr_val_set) in ot_q_gram_attr_val_dict.items():\n","  q_gram_attr_val_set = all_q_gram_attr_val_dict.get(q_gram, set())\n","  q_gram_attr_val_set = q_gram_attr_val_set | attr_val_set\n","  all_q_gram_attr_val_dict[q_gram] = q_gram_attr_val_set\n","\n","num_all_q_gram =    len(all_q_gram_attr_val_dict)\n","num_common_q_gram = len(set(my_q_gram_attr_val_dict.keys()) & \\\n","                        set(ot_q_gram_attr_val_dict.keys()))\n","\n","print('A total of %d unique q-grams occur in both data set' % (num_all_q_gram))\n","print('  Of these, %d occur in both data sets' % (num_common_q_gram))\n","print('')\n","\n","num_common_attr_val = len(set(my_attr_val_q_gram_dict.keys()) & \\\n","                          set(ot_attr_val_q_gram_dict.keys()))\n","num_my_attr_val = len(my_attr_val_q_gram_dict)\n","num_ot_attr_val = len(ot_attr_val_q_gram_dict)\n","\n","jacc_common_attr_val = float(num_common_attr_val) / \\\n","  (num_my_attr_val + num_ot_attr_val - num_common_attr_val)\n","\n","print('My database contains %d unique attribute values' % (num_my_attr_val))\n","print('The other database contains %d unique attribute values' % \\\n","      (num_ot_attr_val))\n","print('  Number of unique attribute values in common: %d' % \\\n","      (num_common_attr_val))\n","print('    Jaccard based overlap of common attribute values: %.1f%%' % \\\n","      (100.0*jacc_common_attr_val))\n","print('')\n","\n","# -----------------------------------------------------------------------------\n","# Step 2: Generate Bloom filters from q-gram sets\n","#\n","if (num_hash_funct in ['opt', 'opt_half', 'opt_quarter']):\n","\n","  # Set number of hash functions to have in average 50% of bits set to 1\n","  # (Linking Sensitive Data book, 2020)\n","  # num_hash_funct = int(math.ceil(0.5 * BF_LEN / \\\n","  #                                math.floor(avrg_num_q_gram)))\n","  #\n","  opt_num_hash_funct = int(round(numpy.log(2.0) * float(bf_len) / \\\n","                                 my_avr_num_q_gram))\n","  if (num_hash_funct == 'opt'):\n","    num_hash_funct = opt_num_hash_funct\n","  elif (num_hash_funct == 'opt_half'):\n","    num_hash_funct = int(round(opt_num_hash_funct / 2.0))\n","  elif (num_hash_funct == 'opt_quarter'):\n","    num_hash_funct = int(round(opt_num_hash_funct / 4.0))\n","  else:\n","    raise Exception(num_hash_funct)\n","\n","my_bf_dict, my_bit_pos_q_gram_dict, my_atom_bf_dict = \\\n","    gen_bloom_filter_dict(my_attr_val_q_gram_dict, hash_type, bf_len,\n","                          num_hash_funct)\n","\n","\n","ot_bf_dict, ot_bit_pos_q_gram_dict, ot_atom_bf_dict = \\\n","    gen_bloom_filter_dict(ot_attr_val_q_gram_dict, hash_type, bf_len,\n","                          num_hash_funct)\n","\n","#rodando ate aqui\n","\n","# Header string for results output\n","#\n","header = '### command_line_call, encoded_attr_name_str,num_bf_to_attack, ' +\\\n","      'hash_type, num_hash_funct, num_my_attr_val, num_ot_attr_val, ' +\\\n","      'num_common_attr_val, jacc_common_perc, my_num_q_gram, ot_num_q_gram, ' +\\\n","      'num_all_q_gram, num_common_q_gram, bf_seg_len, bf_seg_perc, ' +\\\n","      'my_num_q_gram_bf_seg, my_min_num_bit_pos, my_avr_num_bit_pos, ' +\\\n","      'my_med_num_bit_pos, my_max_num_bit_pos, ot_num_q_gram_bf_seg, ' +\\\n","      'ot_min_num_bit_pos, ot_avr_num_bit_pos, ot_med_num_bit_pos, ' +\\\n","      'ot_max_num_bit_pos, atom_num_corr_1_1_attr_matches, ' +\\\n","      'atom_num_corr_1_m_attr_matches, atom_num_wrong_1_matches, ' +\\\n","      'atom_num_wrong_m_matches, atom_num_no_matches, num_reidentified_ids, ' +\\\n","      'num_reidentified_ids_matches, num_reidentified_ids_no_matches, ' +\\\n","      'reidentified_id_list'\n","\n","print(header)\n","\n","if result_file_exists == False:\n","  f.write(header.split('### ')[1]+'\\n')\n","\n","# -----------------------------------------------------------------------------\n","# Step 3: Run the segment attack with different segment percentages\n","#\n","for bf_seg_perc in bf_seg_perc_list:\n","\n","  print('-'*80)\n","  print()\n","\n","  # Generate a string to be printed as the result summary\n","  #\n","  res_str = '### \"' + ' '.join(command_line_call)\n","  res_str = res_str.replace(' , ', ' comma ')  # The separator in input file\n","  res_str = res_str.replace(',', ';') + '\", '\n","\n","  # Details of how many attribute values in the two data sets, and how many\n","  # occur in common in both data sets\n","  #\n","  res_str += '%s, %s, %s, %d, %d, %d, %d, %.1f, ' % (my_encoded_attr_name_str,\n","                                                str(num_bf_to_attack),\n","                                                hash_type,\n","                                                num_hash_funct, num_my_attr_val,\n","                                                num_ot_attr_val,\n","                                                num_common_attr_val,\n","                                                100.0*jacc_common_attr_val)\n","\n","  # Details of how many q-grams in the two data sets, how many in total,\n","  # and how many occur occur in common in both data sets\n","  #\n","  res_str += '%d, %d, %d, %d, ' % (my_num_q_gram, ot_num_q_gram, \\\n","                                   num_all_q_gram, num_common_q_gram)\n","\n","  if hash_type == 'xbf':\n","    bf_seg_len = int((float(bf_len)/2)*bf_seg_perc/100)\n","  else:\n","    bf_seg_len = int(float(bf_len)*bf_seg_perc/100)\n","\n","  # Get the BF segments from the BFs of both data sets\n","  #\n","  my_bf_seg_dict = get_bf_segments(my_bf_dict, bf_seg_len)\n","  ot_bf_seg_dict = get_bf_segments(ot_bf_dict, bf_seg_len)\n","  \n","  my_atom_bf_seg_dict = get_bf_segments(my_atom_bf_dict, bf_seg_len)\n","  \n","  my_num_q_gram_bf_seg, my_min_num_bit_pos, my_avr_num_bit_pos, \\\n","         my_med_num_bit_pos, my_max_num_bit_pos = \\\n","                  bf_segment_get_num_q_gram(my_bit_pos_q_gram_dict,\n","                                            bf_seg_len, num_hash_funct)\n","  ot_num_q_gram_bf_seg, ot_min_num_bit_pos, ot_avr_num_bit_pos, \\\n","         ot_med_num_bit_pos, ot_max_num_bit_pos = \\\n","                  bf_segment_get_num_q_gram(ot_bit_pos_q_gram_dict,\n","                                            bf_seg_len, num_hash_funct)\n","\n","  # Add BF segment information to result string\n","  #\n","  res_str += '%d, %d%%, %d, %d, %.2f, %d, %d, ' % (bf_seg_len, bf_seg_perc, \\\n","                                                 my_num_q_gram_bf_seg, \\\n","                                                 my_min_num_bit_pos, \\\n","                                                 my_avr_num_bit_pos, \\\n","                                                 my_med_num_bit_pos, \\\n","                                                 my_max_num_bit_pos)\n","  res_str += '%d, %d, %.2f, %d, %d, ' % (ot_num_q_gram_bf_seg, \\\n","                                       ot_min_num_bit_pos, \\\n","                                       ot_avr_num_bit_pos, \\\n","                                       ot_med_num_bit_pos, \\\n","                                       ot_max_num_bit_pos)\n","\n","  # Atom based attack\n","  #\n","  num_corr_1_1_attr_matches, num_corr_1_m_attr_matches, \\\n","      num_wrong_1_matches, num_wrong_m_matches, \\\n","      num_no_matches,records_matched = bf_segment_atom_attack(ot_bf_seg_dict,\n","                                              my_atom_bf_seg_dict,\n","                                              my_bit_pos_q_gram_dict,\n","                                              all_q_gram_attr_val_dict,\n","                                              num_bf_to_attack)\n","\n","  res_str += '%d, %d, %d, %d, %d, ' % (num_corr_1_1_attr_matches, \\\n","                                     num_corr_1_m_attr_matches, \\\n","                                     num_wrong_1_matches, num_wrong_m_matches, \\\n","                                     num_no_matches)\n","\n","  # profile \n","  #\n","  num_reidentified_ids,num_reidentified_ids_matches, \\\n","  num_reidentified_ids_no_matches = profile_reidentied_records(DATASET_PATH,\n","                                          my_data_set_name,ot_data_set_name,\n","                                          col_sep_char,rec_id_col, attr_list)\n","  \n","  res_str += '%d, %d, %d,' % (num_reidentified_ids,num_reidentified_ids_matches,\n","                              num_reidentified_ids_no_matches)\n","  \n","  saida_lista = ''\n","  for r_id in records_matched:\n","    saida_lista += str(r_id)+'#'\n","\n","  res_str += '%s' % (saida_lista) \n","  # Print result line for CSV generation\n","  #\n","\n","  print(res_str)\n","  f.write(res_str.split('### ')[1]+'\\n') #savinf csv file\n","  f.flush()\n","\n","f.close()\n"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"Af8FZJUxRBoX","executionInfo":{"status":"ok","timestamp":1630737575481,"user_tz":180,"elapsed":2073768,"user":{"displayName":"Thiago Nóbrega","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_-BHXOY4rTl8cwozc96ukJW5fc7kC04rST8UwDgY=s64","userId":"01534443958959954150"}},"outputId":"17874a98-8244-436c-c82c-3a15d7001dc4"}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{"id":"gHNZvUEnRF6Y"}},{"cell_type":"code","execution_count":null,"source":["#@title Run attack { form-width: \"50%\" }\n","\n","# NUM_MATCHES_TO_TRY = 1000  # or 'all'\n","\n","# initial parameters\n","BF_HASH_FUNCT1 = hashlib.sha1\n","BF_HASH_FUNCT2 = hashlib.md5\n","random.seed(42)\n","\n","DATASET_PATH='/tmp/'\n","try:\n","  RESULTS_PATH\n","except NameError:\n","  RESULTS_PATH='/tmp/'\n","\n","command_line_call = sys.argv\n","\n","\n","#@markdown -----------------\n","#@markdown ##### Dataset:\n","dataset = \"ncvoter-sample-nm-s4\" #@param [\"ncvoter\",\"ncvoter-sample\",\"ncvoter-sample-nm-s2\",\"ncvoter-sample-nm-s3\",\"ncvoter-sample-nm-s4\",\"ncvoter-sample-nm-s5\",\"ncvoter-sample-nm-s6\",\"politicians\",\"politicians-split-2\",\"politicians-split-3\",\"politicians-split-4\",\"politicians-split-5\",\"politicians-split-6\"]\n","\n","if dataset == 'politicians':\n","  my_data_set_name = 'brpoliticians-federal-2014-all-1to1-a.csv.gz'\n","  ot_data_set_name = 'brpoliticians-federal-2018-all-1to1-b.csv.gz'\n","if dataset == 'politicians-split-2':\n","  my_data_set_name = 'brpoliticians-federal-2014-all-1to1-a.csv.gz'\n","  ot_data_set_name = 'brpoliticians-federal-2018-5p-i2.csv.gz'\n","if dataset == 'politicians-split-3':\n","  my_data_set_name = 'brpoliticians-federal-2014-all-1to1-a.csv.gz'\n","  ot_data_set_name = 'brpoliticians-federal-2018-5p-i3.csv.gz'\n","if dataset == 'politicians-split-4':\n","  my_data_set_name = 'brpoliticians-federal-2014-all-1to1-a.csv.gz'\n","  ot_data_set_name = 'brpoliticians-federal-2018-5p-i4.csv.gz'\n","if dataset == 'politicians-split-5':\n","  my_data_set_name = 'brpoliticians-federal-2014-all-1to1-a.csv.gz'\n","  ot_data_set_name = 'brpoliticians-federal-2018-5p-i5.csv.gz'\n","if dataset == 'politicians-split-6':\n","  my_data_set_name = 'brpoliticians-federal-2014-all-1to1-a.csv.gz'\n","  ot_data_set_name = 'brpoliticians-federal-2018-5p-i6.csv.gz'\n","\n","if dataset == 'ncvoter':\n","  my_data_set_name = 'ncvoter-20140619-temporal-balanced-ratio-1to1-a.csv.gz'\n","  ot_data_set_name = 'ncvoter-20140619-temporal-balanced-ratio-1to1-b.csv.gz'\n","if dataset == 'ncvoter-sample':\n","  my_data_set_name = 'ncvoter-20140619-temporal-balanced-ratio-1to1-a.csv.gz'\n","  ot_data_set_name = 'sample-ncvoter-20140619-temporal-balanced-ratio-1to1-b.csv.gz'\n","if dataset == 'ncvoter-sample-nm-s2':\n","  my_data_set_name = 'ncvoter-20140619-temporal-balanced-ratio-1to1-a.csv.gz'\n","  ot_data_set_name = 'sample-ncvoter-20140619-temporal-balanced-ratio-1to1-b-bk_NM-split2.csv.gz'\n","if dataset == 'ncvoter-sample-nm-s3':\n","  my_data_set_name = 'ncvoter-20140619-temporal-balanced-ratio-1to1-a.csv.gz'\n","  ot_data_set_name = 'sample-ncvoter-20140619-temporal-balanced-ratio-1to1-b-bk_NM-split3.csv.gz'\n","if dataset == 'ncvoter-sample-nm-s4':\n","  my_data_set_name = 'ncvoter-20140619-temporal-balanced-ratio-1to1-a.csv.gz'\n","  ot_data_set_name = 'sample-ncvoter-20140619-temporal-balanced-ratio-1to1-b-bk_NM-split4.csv.gz'\n","if dataset == 'ncvoter-sample-nm-s5':\n","  my_data_set_name = 'ncvoter-20140619-temporal-balanced-ratio-1to1-a.csv.gz'\n","  ot_data_set_name = 'sample-ncvoter-20140619-temporal-balanced-ratio-1to1-b-bk_NM-split5.csv.gz'\n","if dataset == 'ncvoter-sample-nm-s6':\n","  my_data_set_name = 'ncvoter-20140619-temporal-balanced-ratio-1to1-a.csv.gz'\n","  ot_data_set_name = 'sample-ncvoter-20140619-temporal-balanced-ratio-1to1-b-bk_NM-split6.csv.gz'\n","\n","rec_id_col   = 0   #@param {type:\"integer\"}\n","\n","#voter_reg_num,name_prefix,first_name,middle_name,last_name,\n","#name_suffix,age,gender,race,ethnic,\n","#street_address,city,state,zip_code,full_phone_num,\n","#birth_place,register_date,download_month\n","#@markdown ###### attr_list parameters\n","#@markdown | ATTRS | POS | ATTRS | POS |\n","#@markdown |:---:|:---:|:---:|:---:|\n","#@markdown |<td colspan=2>NCVR <td colspan=2>BR\n","#@markdown |first_name |3|first_name |1|\n","#@markdown |first_name , last_name|3,5|first_name , last_name|1,2|\n","#@markdown | full_name |3,4,5| full_name |3|\n","#@markdown |first_name , last_name , address|3,5,11|full_name, address|3,9,10|\n","# <sup> NCVR </sup>\n","# > <sup>3,5 - first_name , last_name</sup>\\\n","# > <sup>3,5,11 - first_name , last_name, street_address</sup>\\\n","# > <sup>3,5,6,7,12 - first_name , last_name, age, gender, city</sup>\n","attr_list        =    [3,5]#@param {type:\"raw\"}\n","#@markdown ###### col separator [(',' - ncvr),(';' - brpol)]\n","col_sep_char = ',' #@param {type:\"string\"}[',',';']\n","header_line_flag = True #@param {type:\"raw\"}\n","\n","#@markdown -----------------\n","#@markdown #### Anonymization Parameter:\n","\n","q =  2#@param {type:\"integer\"}\n","hash_type = \"xrh\" #@param [\"rh\",\"xrh\",\"xbf\",\"dh\"]\n","num_hash_funct = \"opt\" #@param [\"opt\", \"opt_half\", \"opt_quarter\"]\n","bf_len =  200#@param {type:\"integer\"}\n","\n","#ajustar depois\n","bf_seg_perc_list = [20] #@param {type:\"raw\"}\n","num_bf_to_attack =  'all'#@param {type:\"raw\"}\n","\n","main_start_time = time.time()\n","\n","# command_line_call = sys.argv\n","# q =                int(sys.argv[1])\n","# hash_type =        sys.argv[2].lower()\n","# num_hash_funct =   sys.argv[3]\n","# bf_len =           int(sys.argv[4])\n","# my_data_set_name = sys.argv[5]\n","# ot_data_set_name = sys.argv[6]\n","# rec_id_col =       int(sys.argv[7])\n","# col_sep_char =     sys.argv[8]\n","# header_line_flag = eval(sys.argv[9])\n","# attr_list =        eval(sys.argv[10])\n","# bf_seg_perc_list = eval(sys.argv[11])\n","# num_bf_to_attack = sys.argv[12]\n","\n","assert q >= 1, q\n","assert hash_type in ['dh','rh','xrh','xbf'], hash_type\n","if num_hash_funct.isdigit():\n","  num_hash_funct = int(num_hash_funct)\n","  assert num_hash_funct >= 1, num_hash_funct\n","else:\n","  assert num_hash_funct in ['opt', 'opt_half', 'opt_quarter'], num_hash_funct\n","assert bf_len > 1, bf_len\n","#\n","assert rec_id_col >= 0, rec_id_col\n","assert header_line_flag in [True,False], header_line_flag\n","assert isinstance(attr_list, list), attr_list\n","assert isinstance(bf_seg_perc_list, list), bf_seg_perc_list\n","if (num_bf_to_attack != 'all'):\n","  num_bf_to_attack = int(num_bf_to_attack)\n","  assert num_bf_to_attack > 1, num_bf_to_attack\n","\n","my_base_data_set_name = my_data_set_name.split('/')[-1].replace('.csv', '')\n","my_base_data_set_name = my_base_data_set_name.replace('.gz','')\n","ot_base_data_set_name = ot_data_set_name.split('/')[-1].replace('.csv', '')\n","ot_base_data_set_name = ot_base_data_set_name.replace('.gz','')\n","\n","assert ',' not in my_base_data_set_name\n","assert ',' not in ot_base_data_set_name\n","\n","print('')\n","print('-'*80)\n","print('')\n","\n","ds_name = my_data_set_name.split('-')[0]\n","res_file = 'real-'+ds_name+'-'+str(num_bf_to_attack)+'-'+str(hash_type)+'-'+str(bf_len)+'.csv'\n","result_file_exists = os.path.exists(RESULTS_PATH + res_file)\n","\n","f = open(RESULTS_PATH + res_file, \"a\")\n","\n","\n","\n","# -----------------------------------------------------------------------------\n","# Step 1: Load the data sets and extract q-grams for selected attributes\n","#\n","my_attr_val_q_gram_dict, my_q_gram_attr_val_dict, my_avr_num_q_gram, \\\n","   my_all_q_gram_set, my_encoded_attr_name_str = \\\n","                       load_data_set_extract_q_grams(my_data_set_name,\n","                                                     rec_id_col, attr_list,\n","                                                     col_sep_char,\n","                                                     header_line_flag, q)\n","\n","ot_attr_val_q_gram_dict, ot_q_gram_attr_val_dict, ot_avr_num_q_gram, \\\n","   ot_all_q_gram_set, ot_encoded_attr_name_str = \\\n","                       load_data_set_extract_q_grams(ot_data_set_name,\n","                                                     rec_id_col, attr_list,\n","                                                     col_sep_char,\n","                                                     header_line_flag, q)\n","\n","assert my_encoded_attr_name_str == ot_encoded_attr_name_str\n","\n","# Generate a dictionary with all q-grams and their attribute values from both\n","# data sets\n","#\n","my_num_q_gram = len(my_q_gram_attr_val_dict)\n","ot_num_q_gram = len(ot_q_gram_attr_val_dict)\n","\n","all_q_gram_attr_val_dict =    {}\n","\n","#modified to python3\n","for (q_gram, attr_val_set) in my_q_gram_attr_val_dict.items():\n","  all_q_gram_attr_val_dict[q_gram] = attr_val_set\n","for (q_gram, attr_val_set) in ot_q_gram_attr_val_dict.items():\n","  q_gram_attr_val_set = all_q_gram_attr_val_dict.get(q_gram, set())\n","  q_gram_attr_val_set = q_gram_attr_val_set | attr_val_set\n","  all_q_gram_attr_val_dict[q_gram] = q_gram_attr_val_set\n","\n","num_all_q_gram =    len(all_q_gram_attr_val_dict)\n","num_common_q_gram = len(set(my_q_gram_attr_val_dict.keys()) & \\\n","                        set(ot_q_gram_attr_val_dict.keys()))\n","\n","print('A total of %d unique q-grams occur in both data set' % (num_all_q_gram))\n","print('  Of these, %d occur in both data sets' % (num_common_q_gram))\n","print('')\n","\n","num_common_attr_val = len(set(my_attr_val_q_gram_dict.keys()) & \\\n","                          set(ot_attr_val_q_gram_dict.keys()))\n","num_my_attr_val = len(my_attr_val_q_gram_dict)\n","num_ot_attr_val = len(ot_attr_val_q_gram_dict)\n","\n","jacc_common_attr_val = float(num_common_attr_val) / \\\n","  (num_my_attr_val + num_ot_attr_val - num_common_attr_val)\n","\n","print('My database contains %d unique attribute values' % (num_my_attr_val))\n","print('The other database contains %d unique attribute values' % \\\n","      (num_ot_attr_val))\n","print('  Number of unique attribute values in common: %d' % \\\n","      (num_common_attr_val))\n","print('    Jaccard based overlap of common attribute values: %.1f%%' % \\\n","      (100.0*jacc_common_attr_val))\n","print('')\n","\n","# -----------------------------------------------------------------------------\n","# Step 2: Generate Bloom filters from q-gram sets\n","#\n","if (num_hash_funct in ['opt', 'opt_half', 'opt_quarter']):\n","\n","  # Set number of hash functions to have in average 50% of bits set to 1\n","  # (Linking Sensitive Data book, 2020)\n","  # num_hash_funct = int(math.ceil(0.5 * BF_LEN / \\\n","  #                                math.floor(avrg_num_q_gram)))\n","  #\n","  opt_num_hash_funct = int(round(numpy.log(2.0) * float(bf_len) / \\\n","                                 my_avr_num_q_gram))\n","  if (num_hash_funct == 'opt'):\n","    num_hash_funct = opt_num_hash_funct\n","  elif (num_hash_funct == 'opt_half'):\n","    num_hash_funct = int(round(opt_num_hash_funct / 2.0))\n","  elif (num_hash_funct == 'opt_quarter'):\n","    num_hash_funct = int(round(opt_num_hash_funct / 4.0))\n","  else:\n","    raise Exception(num_hash_funct)\n","\n","my_bf_dict, my_bit_pos_q_gram_dict, my_atom_bf_dict = \\\n","    gen_bloom_filter_dict(my_attr_val_q_gram_dict, hash_type, bf_len,\n","                          num_hash_funct)\n","\n","\n","ot_bf_dict, ot_bit_pos_q_gram_dict, ot_atom_bf_dict = \\\n","    gen_bloom_filter_dict(ot_attr_val_q_gram_dict, hash_type, bf_len,\n","                          num_hash_funct)\n","\n","#rodando ate aqui\n","\n","# Header string for results output\n","#\n","header = '### command_line_call, encoded_attr_name_str,num_bf_to_attack, ' +\\\n","      'hash_type, num_hash_funct, num_my_attr_val, num_ot_attr_val, ' +\\\n","      'num_common_attr_val, jacc_common_perc, my_num_q_gram, ot_num_q_gram, ' +\\\n","      'num_all_q_gram, num_common_q_gram, bf_seg_len, bf_seg_perc, ' +\\\n","      'my_num_q_gram_bf_seg, my_min_num_bit_pos, my_avr_num_bit_pos, ' +\\\n","      'my_med_num_bit_pos, my_max_num_bit_pos, ot_num_q_gram_bf_seg, ' +\\\n","      'ot_min_num_bit_pos, ot_avr_num_bit_pos, ot_med_num_bit_pos, ' +\\\n","      'ot_max_num_bit_pos, atom_num_corr_1_1_attr_matches, ' +\\\n","      'atom_num_corr_1_m_attr_matches, atom_num_wrong_1_matches, ' +\\\n","      'atom_num_wrong_m_matches, atom_num_no_matches, num_reidentified_ids, ' +\\\n","      'num_reidentified_ids_matches, num_reidentified_ids_no_matches, ' +\\\n","      'reidentified_id_list'\n","\n","print(header)\n","\n","if result_file_exists == False:\n","  f.write(header.split('### ')[1]+'\\n')\n","\n","# -----------------------------------------------------------------------------\n","# Step 3: Run the segment attack with different segment percentages\n","#\n","for bf_seg_perc in bf_seg_perc_list:\n","\n","  print('-'*80)\n","  print()\n","\n","  # Generate a string to be printed as the result summary\n","  #\n","  res_str = '### \"' + ' '.join(command_line_call)\n","  res_str = res_str.replace(' , ', ' comma ')  # The separator in input file\n","  res_str = res_str.replace(',', ';') + '\", '\n","\n","  # Details of how many attribute values in the two data sets, and how many\n","  # occur in common in both data sets\n","  #\n","  res_str += '%s, %s, %s, %d, %d, %d, %d, %.1f, ' % (my_encoded_attr_name_str,\n","                                                str(num_bf_to_attack),\n","                                                hash_type,\n","                                                num_hash_funct, num_my_attr_val,\n","                                                num_ot_attr_val,\n","                                                num_common_attr_val,\n","                                                100.0*jacc_common_attr_val)\n","\n","  # Details of how many q-grams in the two data sets, how many in total,\n","  # and how many occur occur in common in both data sets\n","  #\n","  res_str += '%d, %d, %d, %d, ' % (my_num_q_gram, ot_num_q_gram, \\\n","                                   num_all_q_gram, num_common_q_gram)\n","\n","  if hash_type == 'xbf':\n","    bf_seg_len = int((float(bf_len)/2)*bf_seg_perc/100)\n","  else:\n","    bf_seg_len = int(float(bf_len)*bf_seg_perc/100)\n","\n","  # Get the BF segments from the BFs of both data sets\n","  #\n","  my_bf_seg_dict = get_bf_segments(my_bf_dict, bf_seg_len)\n","  ot_bf_seg_dict = get_bf_segments(ot_bf_dict, bf_seg_len)\n","  \n","  my_atom_bf_seg_dict = get_bf_segments(my_atom_bf_dict, bf_seg_len)\n","  \n","  my_num_q_gram_bf_seg, my_min_num_bit_pos, my_avr_num_bit_pos, \\\n","         my_med_num_bit_pos, my_max_num_bit_pos = \\\n","                  bf_segment_get_num_q_gram(my_bit_pos_q_gram_dict,\n","                                            bf_seg_len, num_hash_funct)\n","  ot_num_q_gram_bf_seg, ot_min_num_bit_pos, ot_avr_num_bit_pos, \\\n","         ot_med_num_bit_pos, ot_max_num_bit_pos = \\\n","                  bf_segment_get_num_q_gram(ot_bit_pos_q_gram_dict,\n","                                            bf_seg_len, num_hash_funct)\n","\n","  # Add BF segment information to result string\n","  #\n","  res_str += '%d, %d%%, %d, %d, %.2f, %d, %d, ' % (bf_seg_len, bf_seg_perc, \\\n","                                                 my_num_q_gram_bf_seg, \\\n","                                                 my_min_num_bit_pos, \\\n","                                                 my_avr_num_bit_pos, \\\n","                                                 my_med_num_bit_pos, \\\n","                                                 my_max_num_bit_pos)\n","  res_str += '%d, %d, %.2f, %d, %d, ' % (ot_num_q_gram_bf_seg, \\\n","                                       ot_min_num_bit_pos, \\\n","                                       ot_avr_num_bit_pos, \\\n","                                       ot_med_num_bit_pos, \\\n","                                       ot_max_num_bit_pos)\n","\n","  # Atom based attack\n","  #\n","  num_corr_1_1_attr_matches, num_corr_1_m_attr_matches, \\\n","      num_wrong_1_matches, num_wrong_m_matches, \\\n","      num_no_matches,records_matched = bf_segment_atom_attack(ot_bf_seg_dict,\n","                                              my_atom_bf_seg_dict,\n","                                              my_bit_pos_q_gram_dict,\n","                                              all_q_gram_attr_val_dict,\n","                                              num_bf_to_attack)\n","\n","  res_str += '%d, %d, %d, %d, %d, ' % (num_corr_1_1_attr_matches, \\\n","                                     num_corr_1_m_attr_matches, \\\n","                                     num_wrong_1_matches, num_wrong_m_matches, \\\n","                                     num_no_matches)\n","\n","  # profile \n","  #\n","  num_reidentified_ids,num_reidentified_ids_matches, \\\n","  num_reidentified_ids_no_matches = profile_reidentied_records(DATASET_PATH,\n","                                          my_data_set_name,ot_data_set_name,\n","                                          col_sep_char,rec_id_col, attr_list)\n","  \n","  res_str += '%d, %d, %d,' % (num_reidentified_ids,num_reidentified_ids_matches,\n","                              num_reidentified_ids_no_matches)\n","  \n","  saida_lista = ''\n","  for r_id in records_matched:\n","    saida_lista += str(r_id)+'#'\n","\n","  res_str += '%s' % (saida_lista) \n","  # Print result line for CSV generation\n","  #\n","\n","  print(res_str)\n","  f.write(res_str.split('### ')[1]+'\\n') #savinf csv file\n","  f.flush()\n","\n","f.close()\n"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"20IQdrzTRGbJ","executionInfo":{"status":"ok","timestamp":1630739055745,"user_tz":180,"elapsed":1480275,"user":{"displayName":"Thiago Nóbrega","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_-BHXOY4rTl8cwozc96ukJW5fc7kC04rST8UwDgY=s64","userId":"01534443958959954150"}},"outputId":"50d873da-0a7b-43a1-c29e-fc2c09e5a505"}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{"id":"t_xQz2W7RLmT"}},{"cell_type":"code","execution_count":null,"source":["#@title Run attack { form-width: \"50%\" }\n","\n","# NUM_MATCHES_TO_TRY = 1000  # or 'all'\n","\n","# initial parameters\n","BF_HASH_FUNCT1 = hashlib.sha1\n","BF_HASH_FUNCT2 = hashlib.md5\n","random.seed(42)\n","\n","DATASET_PATH='/tmp/'\n","try:\n","  RESULTS_PATH\n","except NameError:\n","  RESULTS_PATH='/tmp/'\n","\n","command_line_call = sys.argv\n","\n","\n","#@markdown -----------------\n","#@markdown ##### Dataset:\n","dataset = \"ncvoter-sample-nm-s5\" #@param [\"ncvoter\",\"ncvoter-sample\",\"ncvoter-sample-nm-s2\",\"ncvoter-sample-nm-s3\",\"ncvoter-sample-nm-s4\",\"ncvoter-sample-nm-s5\",\"ncvoter-sample-nm-s6\",\"politicians\",\"politicians-split-2\",\"politicians-split-3\",\"politicians-split-4\",\"politicians-split-5\",\"politicians-split-6\"]\n","\n","if dataset == 'politicians':\n","  my_data_set_name = 'brpoliticians-federal-2014-all-1to1-a.csv.gz'\n","  ot_data_set_name = 'brpoliticians-federal-2018-all-1to1-b.csv.gz'\n","if dataset == 'politicians-split-2':\n","  my_data_set_name = 'brpoliticians-federal-2014-all-1to1-a.csv.gz'\n","  ot_data_set_name = 'brpoliticians-federal-2018-5p-i2.csv.gz'\n","if dataset == 'politicians-split-3':\n","  my_data_set_name = 'brpoliticians-federal-2014-all-1to1-a.csv.gz'\n","  ot_data_set_name = 'brpoliticians-federal-2018-5p-i3.csv.gz'\n","if dataset == 'politicians-split-4':\n","  my_data_set_name = 'brpoliticians-federal-2014-all-1to1-a.csv.gz'\n","  ot_data_set_name = 'brpoliticians-federal-2018-5p-i4.csv.gz'\n","if dataset == 'politicians-split-5':\n","  my_data_set_name = 'brpoliticians-federal-2014-all-1to1-a.csv.gz'\n","  ot_data_set_name = 'brpoliticians-federal-2018-5p-i5.csv.gz'\n","if dataset == 'politicians-split-6':\n","  my_data_set_name = 'brpoliticians-federal-2014-all-1to1-a.csv.gz'\n","  ot_data_set_name = 'brpoliticians-federal-2018-5p-i6.csv.gz'\n","\n","if dataset == 'ncvoter':\n","  my_data_set_name = 'ncvoter-20140619-temporal-balanced-ratio-1to1-a.csv.gz'\n","  ot_data_set_name = 'ncvoter-20140619-temporal-balanced-ratio-1to1-b.csv.gz'\n","if dataset == 'ncvoter-sample':\n","  my_data_set_name = 'ncvoter-20140619-temporal-balanced-ratio-1to1-a.csv.gz'\n","  ot_data_set_name = 'sample-ncvoter-20140619-temporal-balanced-ratio-1to1-b.csv.gz'\n","if dataset == 'ncvoter-sample-nm-s2':\n","  my_data_set_name = 'ncvoter-20140619-temporal-balanced-ratio-1to1-a.csv.gz'\n","  ot_data_set_name = 'sample-ncvoter-20140619-temporal-balanced-ratio-1to1-b-bk_NM-split2.csv.gz'\n","if dataset == 'ncvoter-sample-nm-s3':\n","  my_data_set_name = 'ncvoter-20140619-temporal-balanced-ratio-1to1-a.csv.gz'\n","  ot_data_set_name = 'sample-ncvoter-20140619-temporal-balanced-ratio-1to1-b-bk_NM-split3.csv.gz'\n","if dataset == 'ncvoter-sample-nm-s4':\n","  my_data_set_name = 'ncvoter-20140619-temporal-balanced-ratio-1to1-a.csv.gz'\n","  ot_data_set_name = 'sample-ncvoter-20140619-temporal-balanced-ratio-1to1-b-bk_NM-split4.csv.gz'\n","if dataset == 'ncvoter-sample-nm-s5':\n","  my_data_set_name = 'ncvoter-20140619-temporal-balanced-ratio-1to1-a.csv.gz'\n","  ot_data_set_name = 'sample-ncvoter-20140619-temporal-balanced-ratio-1to1-b-bk_NM-split5.csv.gz'\n","if dataset == 'ncvoter-sample-nm-s6':\n","  my_data_set_name = 'ncvoter-20140619-temporal-balanced-ratio-1to1-a.csv.gz'\n","  ot_data_set_name = 'sample-ncvoter-20140619-temporal-balanced-ratio-1to1-b-bk_NM-split6.csv.gz'\n","\n","rec_id_col   = 0   #@param {type:\"integer\"}\n","\n","#voter_reg_num,name_prefix,first_name,middle_name,last_name,\n","#name_suffix,age,gender,race,ethnic,\n","#street_address,city,state,zip_code,full_phone_num,\n","#birth_place,register_date,download_month\n","#@markdown ###### attr_list parameters\n","#@markdown | ATTRS | POS | ATTRS | POS |\n","#@markdown |:---:|:---:|:---:|:---:|\n","#@markdown |<td colspan=2>NCVR <td colspan=2>BR\n","#@markdown |first_name |3|first_name |1|\n","#@markdown |first_name , last_name|3,5|first_name , last_name|1,2|\n","#@markdown | full_name |3,4,5| full_name |3|\n","#@markdown |first_name , last_name , address|3,5,11|full_name, address|3,9,10|\n","# <sup> NCVR </sup>\n","# > <sup>3,5 - first_name , last_name</sup>\\\n","# > <sup>3,5,11 - first_name , last_name, street_address</sup>\\\n","# > <sup>3,5,6,7,12 - first_name , last_name, age, gender, city</sup>\n","attr_list        =    [3,5]#@param {type:\"raw\"}\n","#@markdown ###### col separator [(',' - ncvr),(';' - brpol)]\n","col_sep_char = ',' #@param {type:\"string\"}[',',';']\n","header_line_flag = True #@param {type:\"raw\"}\n","\n","#@markdown -----------------\n","#@markdown #### Anonymization Parameter:\n","\n","q =  2#@param {type:\"integer\"}\n","hash_type = \"xrh\" #@param [\"rh\",\"xrh\",\"xbf\",\"dh\"]\n","num_hash_funct = \"opt\" #@param [\"opt\", \"opt_half\", \"opt_quarter\"]\n","bf_len =  200#@param {type:\"integer\"}\n","\n","#ajustar depois\n","bf_seg_perc_list = [25] #@param {type:\"raw\"}\n","num_bf_to_attack =  'all'#@param {type:\"raw\"}\n","\n","main_start_time = time.time()\n","\n","# command_line_call = sys.argv\n","# q =                int(sys.argv[1])\n","# hash_type =        sys.argv[2].lower()\n","# num_hash_funct =   sys.argv[3]\n","# bf_len =           int(sys.argv[4])\n","# my_data_set_name = sys.argv[5]\n","# ot_data_set_name = sys.argv[6]\n","# rec_id_col =       int(sys.argv[7])\n","# col_sep_char =     sys.argv[8]\n","# header_line_flag = eval(sys.argv[9])\n","# attr_list =        eval(sys.argv[10])\n","# bf_seg_perc_list = eval(sys.argv[11])\n","# num_bf_to_attack = sys.argv[12]\n","\n","assert q >= 1, q\n","assert hash_type in ['dh','rh','xrh','xbf'], hash_type\n","if num_hash_funct.isdigit():\n","  num_hash_funct = int(num_hash_funct)\n","  assert num_hash_funct >= 1, num_hash_funct\n","else:\n","  assert num_hash_funct in ['opt', 'opt_half', 'opt_quarter'], num_hash_funct\n","assert bf_len > 1, bf_len\n","#\n","assert rec_id_col >= 0, rec_id_col\n","assert header_line_flag in [True,False], header_line_flag\n","assert isinstance(attr_list, list), attr_list\n","assert isinstance(bf_seg_perc_list, list), bf_seg_perc_list\n","if (num_bf_to_attack != 'all'):\n","  num_bf_to_attack = int(num_bf_to_attack)\n","  assert num_bf_to_attack > 1, num_bf_to_attack\n","\n","my_base_data_set_name = my_data_set_name.split('/')[-1].replace('.csv', '')\n","my_base_data_set_name = my_base_data_set_name.replace('.gz','')\n","ot_base_data_set_name = ot_data_set_name.split('/')[-1].replace('.csv', '')\n","ot_base_data_set_name = ot_base_data_set_name.replace('.gz','')\n","\n","assert ',' not in my_base_data_set_name\n","assert ',' not in ot_base_data_set_name\n","\n","print('')\n","print('-'*80)\n","print('')\n","\n","ds_name = my_data_set_name.split('-')[0]\n","res_file = 'real-'+ds_name+'-'+str(num_bf_to_attack)+'-'+str(hash_type)+'-'+str(bf_len)+'.csv'\n","result_file_exists = os.path.exists(RESULTS_PATH + res_file)\n","\n","f = open(RESULTS_PATH + res_file, \"a\")\n","\n","\n","\n","# -----------------------------------------------------------------------------\n","# Step 1: Load the data sets and extract q-grams for selected attributes\n","#\n","my_attr_val_q_gram_dict, my_q_gram_attr_val_dict, my_avr_num_q_gram, \\\n","   my_all_q_gram_set, my_encoded_attr_name_str = \\\n","                       load_data_set_extract_q_grams(my_data_set_name,\n","                                                     rec_id_col, attr_list,\n","                                                     col_sep_char,\n","                                                     header_line_flag, q)\n","\n","ot_attr_val_q_gram_dict, ot_q_gram_attr_val_dict, ot_avr_num_q_gram, \\\n","   ot_all_q_gram_set, ot_encoded_attr_name_str = \\\n","                       load_data_set_extract_q_grams(ot_data_set_name,\n","                                                     rec_id_col, attr_list,\n","                                                     col_sep_char,\n","                                                     header_line_flag, q)\n","\n","assert my_encoded_attr_name_str == ot_encoded_attr_name_str\n","\n","# Generate a dictionary with all q-grams and their attribute values from both\n","# data sets\n","#\n","my_num_q_gram = len(my_q_gram_attr_val_dict)\n","ot_num_q_gram = len(ot_q_gram_attr_val_dict)\n","\n","all_q_gram_attr_val_dict =    {}\n","\n","#modified to python3\n","for (q_gram, attr_val_set) in my_q_gram_attr_val_dict.items():\n","  all_q_gram_attr_val_dict[q_gram] = attr_val_set\n","for (q_gram, attr_val_set) in ot_q_gram_attr_val_dict.items():\n","  q_gram_attr_val_set = all_q_gram_attr_val_dict.get(q_gram, set())\n","  q_gram_attr_val_set = q_gram_attr_val_set | attr_val_set\n","  all_q_gram_attr_val_dict[q_gram] = q_gram_attr_val_set\n","\n","num_all_q_gram =    len(all_q_gram_attr_val_dict)\n","num_common_q_gram = len(set(my_q_gram_attr_val_dict.keys()) & \\\n","                        set(ot_q_gram_attr_val_dict.keys()))\n","\n","print('A total of %d unique q-grams occur in both data set' % (num_all_q_gram))\n","print('  Of these, %d occur in both data sets' % (num_common_q_gram))\n","print('')\n","\n","num_common_attr_val = len(set(my_attr_val_q_gram_dict.keys()) & \\\n","                          set(ot_attr_val_q_gram_dict.keys()))\n","num_my_attr_val = len(my_attr_val_q_gram_dict)\n","num_ot_attr_val = len(ot_attr_val_q_gram_dict)\n","\n","jacc_common_attr_val = float(num_common_attr_val) / \\\n","  (num_my_attr_val + num_ot_attr_val - num_common_attr_val)\n","\n","print('My database contains %d unique attribute values' % (num_my_attr_val))\n","print('The other database contains %d unique attribute values' % \\\n","      (num_ot_attr_val))\n","print('  Number of unique attribute values in common: %d' % \\\n","      (num_common_attr_val))\n","print('    Jaccard based overlap of common attribute values: %.1f%%' % \\\n","      (100.0*jacc_common_attr_val))\n","print('')\n","\n","# -----------------------------------------------------------------------------\n","# Step 2: Generate Bloom filters from q-gram sets\n","#\n","if (num_hash_funct in ['opt', 'opt_half', 'opt_quarter']):\n","\n","  # Set number of hash functions to have in average 50% of bits set to 1\n","  # (Linking Sensitive Data book, 2020)\n","  # num_hash_funct = int(math.ceil(0.5 * BF_LEN / \\\n","  #                                math.floor(avrg_num_q_gram)))\n","  #\n","  opt_num_hash_funct = int(round(numpy.log(2.0) * float(bf_len) / \\\n","                                 my_avr_num_q_gram))\n","  if (num_hash_funct == 'opt'):\n","    num_hash_funct = opt_num_hash_funct\n","  elif (num_hash_funct == 'opt_half'):\n","    num_hash_funct = int(round(opt_num_hash_funct / 2.0))\n","  elif (num_hash_funct == 'opt_quarter'):\n","    num_hash_funct = int(round(opt_num_hash_funct / 4.0))\n","  else:\n","    raise Exception(num_hash_funct)\n","\n","my_bf_dict, my_bit_pos_q_gram_dict, my_atom_bf_dict = \\\n","    gen_bloom_filter_dict(my_attr_val_q_gram_dict, hash_type, bf_len,\n","                          num_hash_funct)\n","\n","\n","ot_bf_dict, ot_bit_pos_q_gram_dict, ot_atom_bf_dict = \\\n","    gen_bloom_filter_dict(ot_attr_val_q_gram_dict, hash_type, bf_len,\n","                          num_hash_funct)\n","\n","#rodando ate aqui\n","\n","# Header string for results output\n","#\n","header = '### command_line_call, encoded_attr_name_str,num_bf_to_attack, ' +\\\n","      'hash_type, num_hash_funct, num_my_attr_val, num_ot_attr_val, ' +\\\n","      'num_common_attr_val, jacc_common_perc, my_num_q_gram, ot_num_q_gram, ' +\\\n","      'num_all_q_gram, num_common_q_gram, bf_seg_len, bf_seg_perc, ' +\\\n","      'my_num_q_gram_bf_seg, my_min_num_bit_pos, my_avr_num_bit_pos, ' +\\\n","      'my_med_num_bit_pos, my_max_num_bit_pos, ot_num_q_gram_bf_seg, ' +\\\n","      'ot_min_num_bit_pos, ot_avr_num_bit_pos, ot_med_num_bit_pos, ' +\\\n","      'ot_max_num_bit_pos, atom_num_corr_1_1_attr_matches, ' +\\\n","      'atom_num_corr_1_m_attr_matches, atom_num_wrong_1_matches, ' +\\\n","      'atom_num_wrong_m_matches, atom_num_no_matches, num_reidentified_ids, ' +\\\n","      'num_reidentified_ids_matches, num_reidentified_ids_no_matches, ' +\\\n","      'reidentified_id_list'\n","\n","print(header)\n","\n","if result_file_exists == False:\n","  f.write(header.split('### ')[1]+'\\n')\n","\n","# -----------------------------------------------------------------------------\n","# Step 3: Run the segment attack with different segment percentages\n","#\n","for bf_seg_perc in bf_seg_perc_list:\n","\n","  print('-'*80)\n","  print()\n","\n","  # Generate a string to be printed as the result summary\n","  #\n","  res_str = '### \"' + ' '.join(command_line_call)\n","  res_str = res_str.replace(' , ', ' comma ')  # The separator in input file\n","  res_str = res_str.replace(',', ';') + '\", '\n","\n","  # Details of how many attribute values in the two data sets, and how many\n","  # occur in common in both data sets\n","  #\n","  res_str += '%s, %s, %s, %d, %d, %d, %d, %.1f, ' % (my_encoded_attr_name_str,\n","                                                str(num_bf_to_attack),\n","                                                hash_type,\n","                                                num_hash_funct, num_my_attr_val,\n","                                                num_ot_attr_val,\n","                                                num_common_attr_val,\n","                                                100.0*jacc_common_attr_val)\n","\n","  # Details of how many q-grams in the two data sets, how many in total,\n","  # and how many occur occur in common in both data sets\n","  #\n","  res_str += '%d, %d, %d, %d, ' % (my_num_q_gram, ot_num_q_gram, \\\n","                                   num_all_q_gram, num_common_q_gram)\n","\n","  if hash_type == 'xbf':\n","    bf_seg_len = int((float(bf_len)/2)*bf_seg_perc/100)\n","  else:\n","    bf_seg_len = int(float(bf_len)*bf_seg_perc/100)\n","\n","  # Get the BF segments from the BFs of both data sets\n","  #\n","  my_bf_seg_dict = get_bf_segments(my_bf_dict, bf_seg_len)\n","  ot_bf_seg_dict = get_bf_segments(ot_bf_dict, bf_seg_len)\n","  \n","  my_atom_bf_seg_dict = get_bf_segments(my_atom_bf_dict, bf_seg_len)\n","  \n","  my_num_q_gram_bf_seg, my_min_num_bit_pos, my_avr_num_bit_pos, \\\n","         my_med_num_bit_pos, my_max_num_bit_pos = \\\n","                  bf_segment_get_num_q_gram(my_bit_pos_q_gram_dict,\n","                                            bf_seg_len, num_hash_funct)\n","  ot_num_q_gram_bf_seg, ot_min_num_bit_pos, ot_avr_num_bit_pos, \\\n","         ot_med_num_bit_pos, ot_max_num_bit_pos = \\\n","                  bf_segment_get_num_q_gram(ot_bit_pos_q_gram_dict,\n","                                            bf_seg_len, num_hash_funct)\n","\n","  # Add BF segment information to result string\n","  #\n","  res_str += '%d, %d%%, %d, %d, %.2f, %d, %d, ' % (bf_seg_len, bf_seg_perc, \\\n","                                                 my_num_q_gram_bf_seg, \\\n","                                                 my_min_num_bit_pos, \\\n","                                                 my_avr_num_bit_pos, \\\n","                                                 my_med_num_bit_pos, \\\n","                                                 my_max_num_bit_pos)\n","  res_str += '%d, %d, %.2f, %d, %d, ' % (ot_num_q_gram_bf_seg, \\\n","                                       ot_min_num_bit_pos, \\\n","                                       ot_avr_num_bit_pos, \\\n","                                       ot_med_num_bit_pos, \\\n","                                       ot_max_num_bit_pos)\n","\n","  # Atom based attack\n","  #\n","  num_corr_1_1_attr_matches, num_corr_1_m_attr_matches, \\\n","      num_wrong_1_matches, num_wrong_m_matches, \\\n","      num_no_matches,records_matched = bf_segment_atom_attack(ot_bf_seg_dict,\n","                                              my_atom_bf_seg_dict,\n","                                              my_bit_pos_q_gram_dict,\n","                                              all_q_gram_attr_val_dict,\n","                                              num_bf_to_attack)\n","\n","  res_str += '%d, %d, %d, %d, %d, ' % (num_corr_1_1_attr_matches, \\\n","                                     num_corr_1_m_attr_matches, \\\n","                                     num_wrong_1_matches, num_wrong_m_matches, \\\n","                                     num_no_matches)\n","\n","  # profile \n","  #\n","  num_reidentified_ids,num_reidentified_ids_matches, \\\n","  num_reidentified_ids_no_matches = profile_reidentied_records(DATASET_PATH,\n","                                          my_data_set_name,ot_data_set_name,\n","                                          col_sep_char,rec_id_col, attr_list)\n","  \n","  res_str += '%d, %d, %d,' % (num_reidentified_ids,num_reidentified_ids_matches,\n","                              num_reidentified_ids_no_matches)\n","  \n","  saida_lista = ''\n","  for r_id in records_matched:\n","    saida_lista += str(r_id)+'#'\n","\n","  res_str += '%s' % (saida_lista) \n","  # Print result line for CSV generation\n","  #\n","\n","  print(res_str)\n","  f.write(res_str.split('### ')[1]+'\\n') #savinf csv file\n","  f.flush()\n","\n","f.close()\n"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"sT3RvL7kRMVL","executionInfo":{"status":"ok","timestamp":1630740197821,"user_tz":180,"elapsed":1142086,"user":{"displayName":"Thiago Nóbrega","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_-BHXOY4rTl8cwozc96ukJW5fc7kC04rST8UwDgY=s64","userId":"01534443958959954150"}},"outputId":"ec2e2621-aa0b-4a6d-da23-0c88f55a0c72"}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{"id":"y9SlRmK7RRgz"}},{"cell_type":"code","execution_count":null,"source":["#@title Run attack { form-width: \"50%\" }\n","\n","# NUM_MATCHES_TO_TRY = 1000  # or 'all'\n","\n","# initial parameters\n","BF_HASH_FUNCT1 = hashlib.sha1\n","BF_HASH_FUNCT2 = hashlib.md5\n","random.seed(42)\n","\n","DATASET_PATH='/tmp/'\n","try:\n","  RESULTS_PATH\n","except NameError:\n","  RESULTS_PATH='/tmp/'\n","\n","command_line_call = sys.argv\n","\n","\n","#@markdown -----------------\n","#@markdown ##### Dataset:\n","dataset = \"ncvoter-sample-nm-s6\" #@param [\"ncvoter\",\"ncvoter-sample\",\"ncvoter-sample-nm-s2\",\"ncvoter-sample-nm-s3\",\"ncvoter-sample-nm-s4\",\"ncvoter-sample-nm-s5\",\"ncvoter-sample-nm-s6\",\"politicians\",\"politicians-split-2\",\"politicians-split-3\",\"politicians-split-4\",\"politicians-split-5\",\"politicians-split-6\"]\n","\n","if dataset == 'politicians':\n","  my_data_set_name = 'brpoliticians-federal-2014-all-1to1-a.csv.gz'\n","  ot_data_set_name = 'brpoliticians-federal-2018-all-1to1-b.csv.gz'\n","if dataset == 'politicians-split-2':\n","  my_data_set_name = 'brpoliticians-federal-2014-all-1to1-a.csv.gz'\n","  ot_data_set_name = 'brpoliticians-federal-2018-5p-i2.csv.gz'\n","if dataset == 'politicians-split-3':\n","  my_data_set_name = 'brpoliticians-federal-2014-all-1to1-a.csv.gz'\n","  ot_data_set_name = 'brpoliticians-federal-2018-5p-i3.csv.gz'\n","if dataset == 'politicians-split-4':\n","  my_data_set_name = 'brpoliticians-federal-2014-all-1to1-a.csv.gz'\n","  ot_data_set_name = 'brpoliticians-federal-2018-5p-i4.csv.gz'\n","if dataset == 'politicians-split-5':\n","  my_data_set_name = 'brpoliticians-federal-2014-all-1to1-a.csv.gz'\n","  ot_data_set_name = 'brpoliticians-federal-2018-5p-i5.csv.gz'\n","if dataset == 'politicians-split-6':\n","  my_data_set_name = 'brpoliticians-federal-2014-all-1to1-a.csv.gz'\n","  ot_data_set_name = 'brpoliticians-federal-2018-5p-i6.csv.gz'\n","\n","if dataset == 'ncvoter':\n","  my_data_set_name = 'ncvoter-20140619-temporal-balanced-ratio-1to1-a.csv.gz'\n","  ot_data_set_name = 'ncvoter-20140619-temporal-balanced-ratio-1to1-b.csv.gz'\n","if dataset == 'ncvoter-sample':\n","  my_data_set_name = 'ncvoter-20140619-temporal-balanced-ratio-1to1-a.csv.gz'\n","  ot_data_set_name = 'sample-ncvoter-20140619-temporal-balanced-ratio-1to1-b.csv.gz'\n","if dataset == 'ncvoter-sample-nm-s2':\n","  my_data_set_name = 'ncvoter-20140619-temporal-balanced-ratio-1to1-a.csv.gz'\n","  ot_data_set_name = 'sample-ncvoter-20140619-temporal-balanced-ratio-1to1-b-bk_NM-split2.csv.gz'\n","if dataset == 'ncvoter-sample-nm-s3':\n","  my_data_set_name = 'ncvoter-20140619-temporal-balanced-ratio-1to1-a.csv.gz'\n","  ot_data_set_name = 'sample-ncvoter-20140619-temporal-balanced-ratio-1to1-b-bk_NM-split3.csv.gz'\n","if dataset == 'ncvoter-sample-nm-s4':\n","  my_data_set_name = 'ncvoter-20140619-temporal-balanced-ratio-1to1-a.csv.gz'\n","  ot_data_set_name = 'sample-ncvoter-20140619-temporal-balanced-ratio-1to1-b-bk_NM-split4.csv.gz'\n","if dataset == 'ncvoter-sample-nm-s5':\n","  my_data_set_name = 'ncvoter-20140619-temporal-balanced-ratio-1to1-a.csv.gz'\n","  ot_data_set_name = 'sample-ncvoter-20140619-temporal-balanced-ratio-1to1-b-bk_NM-split5.csv.gz'\n","if dataset == 'ncvoter-sample-nm-s6':\n","  my_data_set_name = 'ncvoter-20140619-temporal-balanced-ratio-1to1-a.csv.gz'\n","  ot_data_set_name = 'sample-ncvoter-20140619-temporal-balanced-ratio-1to1-b-bk_NM-split6.csv.gz'\n","\n","rec_id_col   = 0   #@param {type:\"integer\"}\n","\n","#voter_reg_num,name_prefix,first_name,middle_name,last_name,\n","#name_suffix,age,gender,race,ethnic,\n","#street_address,city,state,zip_code,full_phone_num,\n","#birth_place,register_date,download_month\n","#@markdown ###### attr_list parameters\n","#@markdown | ATTRS | POS | ATTRS | POS |\n","#@markdown |:---:|:---:|:---:|:---:|\n","#@markdown |<td colspan=2>NCVR <td colspan=2>BR\n","#@markdown |first_name |3|first_name |1|\n","#@markdown |first_name , last_name|3,5|first_name , last_name|1,2|\n","#@markdown | full_name |3,4,5| full_name |3|\n","#@markdown |first_name , last_name , address|3,5,11|full_name, address|3,9,10|\n","# <sup> NCVR </sup>\n","# > <sup>3,5 - first_name , last_name</sup>\\\n","# > <sup>3,5,11 - first_name , last_name, street_address</sup>\\\n","# > <sup>3,5,6,7,12 - first_name , last_name, age, gender, city</sup>\n","attr_list        =    [3,5]#@param {type:\"raw\"}\n","#@markdown ###### col separator [(',' - ncvr),(';' - brpol)]\n","col_sep_char = ',' #@param {type:\"string\"}[',',';']\n","header_line_flag = True #@param {type:\"raw\"}\n","\n","#@markdown -----------------\n","#@markdown #### Anonymization Parameter:\n","\n","q =  2#@param {type:\"integer\"}\n","hash_type = \"xrh\" #@param [\"rh\",\"xrh\",\"xbf\",\"dh\"]\n","num_hash_funct = \"opt\" #@param [\"opt\", \"opt_half\", \"opt_quarter\"]\n","bf_len =  200#@param {type:\"integer\"}\n","\n","#ajustar depois\n","bf_seg_perc_list = [100] #@param {type:\"raw\"}\n","num_bf_to_attack =  'all'#@param {type:\"raw\"}\n","\n","main_start_time = time.time()\n","\n","# command_line_call = sys.argv\n","# q =                int(sys.argv[1])\n","# hash_type =        sys.argv[2].lower()\n","# num_hash_funct =   sys.argv[3]\n","# bf_len =           int(sys.argv[4])\n","# my_data_set_name = sys.argv[5]\n","# ot_data_set_name = sys.argv[6]\n","# rec_id_col =       int(sys.argv[7])\n","# col_sep_char =     sys.argv[8]\n","# header_line_flag = eval(sys.argv[9])\n","# attr_list =        eval(sys.argv[10])\n","# bf_seg_perc_list = eval(sys.argv[11])\n","# num_bf_to_attack = sys.argv[12]\n","\n","assert q >= 1, q\n","assert hash_type in ['dh','rh','xrh','xbf'], hash_type\n","if num_hash_funct.isdigit():\n","  num_hash_funct = int(num_hash_funct)\n","  assert num_hash_funct >= 1, num_hash_funct\n","else:\n","  assert num_hash_funct in ['opt', 'opt_half', 'opt_quarter'], num_hash_funct\n","assert bf_len > 1, bf_len\n","#\n","assert rec_id_col >= 0, rec_id_col\n","assert header_line_flag in [True,False], header_line_flag\n","assert isinstance(attr_list, list), attr_list\n","assert isinstance(bf_seg_perc_list, list), bf_seg_perc_list\n","if (num_bf_to_attack != 'all'):\n","  num_bf_to_attack = int(num_bf_to_attack)\n","  assert num_bf_to_attack > 1, num_bf_to_attack\n","\n","my_base_data_set_name = my_data_set_name.split('/')[-1].replace('.csv', '')\n","my_base_data_set_name = my_base_data_set_name.replace('.gz','')\n","ot_base_data_set_name = ot_data_set_name.split('/')[-1].replace('.csv', '')\n","ot_base_data_set_name = ot_base_data_set_name.replace('.gz','')\n","\n","assert ',' not in my_base_data_set_name\n","assert ',' not in ot_base_data_set_name\n","\n","print('')\n","print('-'*80)\n","print('')\n","\n","ds_name = my_data_set_name.split('-')[0]\n","res_file = 'real-'+ds_name+'-'+str(num_bf_to_attack)+'-'+str(hash_type)+'-'+str(bf_len)+'.csv'\n","result_file_exists = os.path.exists(RESULTS_PATH + res_file)\n","\n","f = open(RESULTS_PATH + res_file, \"a\")\n","\n","\n","\n","# -----------------------------------------------------------------------------\n","# Step 1: Load the data sets and extract q-grams for selected attributes\n","#\n","my_attr_val_q_gram_dict, my_q_gram_attr_val_dict, my_avr_num_q_gram, \\\n","   my_all_q_gram_set, my_encoded_attr_name_str = \\\n","                       load_data_set_extract_q_grams(my_data_set_name,\n","                                                     rec_id_col, attr_list,\n","                                                     col_sep_char,\n","                                                     header_line_flag, q)\n","\n","ot_attr_val_q_gram_dict, ot_q_gram_attr_val_dict, ot_avr_num_q_gram, \\\n","   ot_all_q_gram_set, ot_encoded_attr_name_str = \\\n","                       load_data_set_extract_q_grams(ot_data_set_name,\n","                                                     rec_id_col, attr_list,\n","                                                     col_sep_char,\n","                                                     header_line_flag, q)\n","\n","assert my_encoded_attr_name_str == ot_encoded_attr_name_str\n","\n","# Generate a dictionary with all q-grams and their attribute values from both\n","# data sets\n","#\n","my_num_q_gram = len(my_q_gram_attr_val_dict)\n","ot_num_q_gram = len(ot_q_gram_attr_val_dict)\n","\n","all_q_gram_attr_val_dict =    {}\n","\n","#modified to python3\n","for (q_gram, attr_val_set) in my_q_gram_attr_val_dict.items():\n","  all_q_gram_attr_val_dict[q_gram] = attr_val_set\n","for (q_gram, attr_val_set) in ot_q_gram_attr_val_dict.items():\n","  q_gram_attr_val_set = all_q_gram_attr_val_dict.get(q_gram, set())\n","  q_gram_attr_val_set = q_gram_attr_val_set | attr_val_set\n","  all_q_gram_attr_val_dict[q_gram] = q_gram_attr_val_set\n","\n","num_all_q_gram =    len(all_q_gram_attr_val_dict)\n","num_common_q_gram = len(set(my_q_gram_attr_val_dict.keys()) & \\\n","                        set(ot_q_gram_attr_val_dict.keys()))\n","\n","print('A total of %d unique q-grams occur in both data set' % (num_all_q_gram))\n","print('  Of these, %d occur in both data sets' % (num_common_q_gram))\n","print('')\n","\n","num_common_attr_val = len(set(my_attr_val_q_gram_dict.keys()) & \\\n","                          set(ot_attr_val_q_gram_dict.keys()))\n","num_my_attr_val = len(my_attr_val_q_gram_dict)\n","num_ot_attr_val = len(ot_attr_val_q_gram_dict)\n","\n","jacc_common_attr_val = float(num_common_attr_val) / \\\n","  (num_my_attr_val + num_ot_attr_val - num_common_attr_val)\n","\n","print('My database contains %d unique attribute values' % (num_my_attr_val))\n","print('The other database contains %d unique attribute values' % \\\n","      (num_ot_attr_val))\n","print('  Number of unique attribute values in common: %d' % \\\n","      (num_common_attr_val))\n","print('    Jaccard based overlap of common attribute values: %.1f%%' % \\\n","      (100.0*jacc_common_attr_val))\n","print('')\n","\n","# -----------------------------------------------------------------------------\n","# Step 2: Generate Bloom filters from q-gram sets\n","#\n","if (num_hash_funct in ['opt', 'opt_half', 'opt_quarter']):\n","\n","  # Set number of hash functions to have in average 50% of bits set to 1\n","  # (Linking Sensitive Data book, 2020)\n","  # num_hash_funct = int(math.ceil(0.5 * BF_LEN / \\\n","  #                                math.floor(avrg_num_q_gram)))\n","  #\n","  opt_num_hash_funct = int(round(numpy.log(2.0) * float(bf_len) / \\\n","                                 my_avr_num_q_gram))\n","  if (num_hash_funct == 'opt'):\n","    num_hash_funct = opt_num_hash_funct\n","  elif (num_hash_funct == 'opt_half'):\n","    num_hash_funct = int(round(opt_num_hash_funct / 2.0))\n","  elif (num_hash_funct == 'opt_quarter'):\n","    num_hash_funct = int(round(opt_num_hash_funct / 4.0))\n","  else:\n","    raise Exception(num_hash_funct)\n","\n","my_bf_dict, my_bit_pos_q_gram_dict, my_atom_bf_dict = \\\n","    gen_bloom_filter_dict(my_attr_val_q_gram_dict, hash_type, bf_len,\n","                          num_hash_funct)\n","\n","\n","ot_bf_dict, ot_bit_pos_q_gram_dict, ot_atom_bf_dict = \\\n","    gen_bloom_filter_dict(ot_attr_val_q_gram_dict, hash_type, bf_len,\n","                          num_hash_funct)\n","\n","#rodando ate aqui\n","\n","# Header string for results output\n","#\n","header = '### command_line_call, encoded_attr_name_str,num_bf_to_attack, ' +\\\n","      'hash_type, num_hash_funct, num_my_attr_val, num_ot_attr_val, ' +\\\n","      'num_common_attr_val, jacc_common_perc, my_num_q_gram, ot_num_q_gram, ' +\\\n","      'num_all_q_gram, num_common_q_gram, bf_seg_len, bf_seg_perc, ' +\\\n","      'my_num_q_gram_bf_seg, my_min_num_bit_pos, my_avr_num_bit_pos, ' +\\\n","      'my_med_num_bit_pos, my_max_num_bit_pos, ot_num_q_gram_bf_seg, ' +\\\n","      'ot_min_num_bit_pos, ot_avr_num_bit_pos, ot_med_num_bit_pos, ' +\\\n","      'ot_max_num_bit_pos, atom_num_corr_1_1_attr_matches, ' +\\\n","      'atom_num_corr_1_m_attr_matches, atom_num_wrong_1_matches, ' +\\\n","      'atom_num_wrong_m_matches, atom_num_no_matches, num_reidentified_ids, ' +\\\n","      'num_reidentified_ids_matches, num_reidentified_ids_no_matches, ' +\\\n","      'reidentified_id_list'\n","\n","print(header)\n","\n","if result_file_exists == False:\n","  f.write(header.split('### ')[1]+'\\n')\n","\n","# -----------------------------------------------------------------------------\n","# Step 3: Run the segment attack with different segment percentages\n","#\n","for bf_seg_perc in bf_seg_perc_list:\n","\n","  print('-'*80)\n","  print()\n","\n","  # Generate a string to be printed as the result summary\n","  #\n","  res_str = '### \"' + ' '.join(command_line_call)\n","  res_str = res_str.replace(' , ', ' comma ')  # The separator in input file\n","  res_str = res_str.replace(',', ';') + '\", '\n","\n","  # Details of how many attribute values in the two data sets, and how many\n","  # occur in common in both data sets\n","  #\n","  res_str += '%s, %s, %s, %d, %d, %d, %d, %.1f, ' % (my_encoded_attr_name_str,\n","                                                str(num_bf_to_attack),\n","                                                hash_type,\n","                                                num_hash_funct, num_my_attr_val,\n","                                                num_ot_attr_val,\n","                                                num_common_attr_val,\n","                                                100.0*jacc_common_attr_val)\n","\n","  # Details of how many q-grams in the two data sets, how many in total,\n","  # and how many occur occur in common in both data sets\n","  #\n","  res_str += '%d, %d, %d, %d, ' % (my_num_q_gram, ot_num_q_gram, \\\n","                                   num_all_q_gram, num_common_q_gram)\n","\n","  if hash_type == 'xbf':\n","    bf_seg_len = int((float(bf_len)/2)*bf_seg_perc/100)\n","  else:\n","    bf_seg_len = int(float(bf_len)*bf_seg_perc/100)\n","\n","  # Get the BF segments from the BFs of both data sets\n","  #\n","  my_bf_seg_dict = get_bf_segments(my_bf_dict, bf_seg_len)\n","  ot_bf_seg_dict = get_bf_segments(ot_bf_dict, bf_seg_len)\n","  \n","  my_atom_bf_seg_dict = get_bf_segments(my_atom_bf_dict, bf_seg_len)\n","  \n","  my_num_q_gram_bf_seg, my_min_num_bit_pos, my_avr_num_bit_pos, \\\n","         my_med_num_bit_pos, my_max_num_bit_pos = \\\n","                  bf_segment_get_num_q_gram(my_bit_pos_q_gram_dict,\n","                                            bf_seg_len, num_hash_funct)\n","  ot_num_q_gram_bf_seg, ot_min_num_bit_pos, ot_avr_num_bit_pos, \\\n","         ot_med_num_bit_pos, ot_max_num_bit_pos = \\\n","                  bf_segment_get_num_q_gram(ot_bit_pos_q_gram_dict,\n","                                            bf_seg_len, num_hash_funct)\n","\n","  # Add BF segment information to result string\n","  #\n","  res_str += '%d, %d%%, %d, %d, %.2f, %d, %d, ' % (bf_seg_len, bf_seg_perc, \\\n","                                                 my_num_q_gram_bf_seg, \\\n","                                                 my_min_num_bit_pos, \\\n","                                                 my_avr_num_bit_pos, \\\n","                                                 my_med_num_bit_pos, \\\n","                                                 my_max_num_bit_pos)\n","  res_str += '%d, %d, %.2f, %d, %d, ' % (ot_num_q_gram_bf_seg, \\\n","                                       ot_min_num_bit_pos, \\\n","                                       ot_avr_num_bit_pos, \\\n","                                       ot_med_num_bit_pos, \\\n","                                       ot_max_num_bit_pos)\n","\n","  # Atom based attack\n","  #\n","  num_corr_1_1_attr_matches, num_corr_1_m_attr_matches, \\\n","      num_wrong_1_matches, num_wrong_m_matches, \\\n","      num_no_matches,records_matched = bf_segment_atom_attack(ot_bf_seg_dict,\n","                                              my_atom_bf_seg_dict,\n","                                              my_bit_pos_q_gram_dict,\n","                                              all_q_gram_attr_val_dict,\n","                                              num_bf_to_attack)\n","\n","  res_str += '%d, %d, %d, %d, %d, ' % (num_corr_1_1_attr_matches, \\\n","                                     num_corr_1_m_attr_matches, \\\n","                                     num_wrong_1_matches, num_wrong_m_matches, \\\n","                                     num_no_matches)\n","\n","  # profile \n","  #\n","  num_reidentified_ids,num_reidentified_ids_matches, \\\n","  num_reidentified_ids_no_matches = profile_reidentied_records(DATASET_PATH,\n","                                          my_data_set_name,ot_data_set_name,\n","                                          col_sep_char,rec_id_col, attr_list)\n","  \n","  res_str += '%d, %d, %d,' % (num_reidentified_ids,num_reidentified_ids_matches,\n","                              num_reidentified_ids_no_matches)\n","  \n","  saida_lista = ''\n","  for r_id in records_matched:\n","    saida_lista += str(r_id)+'#'\n","\n","  res_str += '%s' % (saida_lista) \n","  # Print result line for CSV generation\n","  #\n","\n","  print(res_str)\n","  f.write(res_str.split('### ')[1]+'\\n') #savinf csv file\n","  f.flush()\n","\n","f.close()\n"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"Yw1CZGEIRTBI","executionInfo":{"status":"ok","timestamp":1630740559537,"user_tz":180,"elapsed":361720,"user":{"displayName":"Thiago Nóbrega","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_-BHXOY4rTl8cwozc96ukJW5fc7kC04rST8UwDgY=s64","userId":"01534443958959954150"}},"outputId":"be23cbba-0f51-4a88-ec9e-b953bd7d1015"}}]}